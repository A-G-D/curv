* Recursive functions
  * Special cases:
    * Recursive static functions (non closures), C function equivalency.
    * supported: Self-recursive function.
    * supported: Mutually recursive functions in the same let or module scope.
    * causes an error: any other kind of recursive definition.
  * RFunctions: All of the functions in a mutual-recursion group (within a
    let or module scope) share the same nonlocal slot vector. These functions
    reference themselves and each other, not by Constant nodes, but by
    nonlocal references, and the slots themselves contain RFunction refs,
    not proper Values. The shared slot vector is represented as a List and
    stored in an additional slot associated with the let or module expression.
    * Hence, recursive functions are not Constants.
    * An RFun reference is a special Meaning class that contains the slot ids
      of the RFun code reference and the nonlocals list. There are local and
      nonlocal variants. When eval'ed, an RFun reference fetches the contents
      of the two slots and constructs a closure Value.
    * Future optimization: use a special operation to call a function via an
      RFun ref without constructing the closure as a temporary Value.
      (Peephole optimization: super-operator.)
  * How to analyze a recursive function.
    * Analysis of a phrase tree is a depth-first recursive process.
      Previously, I map a let/module bound identifier to a slot reference
      with no further analysis. Now, I recursively analyze the definiens phrase,
      and store the resulting Expression.
      * If it is already analyzed, then it's a thunk or rfun.
        Return the corresponding ref type.
        Future optimization: add a third category, proper value.
      * If is an unanalyzed phrase, then mark the symbol table entry as pending,
        then analyze the phrase.
      * If it is a pending phrase, then if a lambda phrase, mark it as an RFun,
        otherwise mark it as a Thunk. analyze() returns a slot reference.
      * Future optimization: add a fourth category, Constant.
    * RFuns in the same mutual recursion group must share a nonlocals list,
      otherwise, if they have separate lists, then there will be reference
      cycles between these lists.
    * Therefore, after analyzing all of the definitions in a let/module scope,
      visit all of the RFun bindings, amalgamate all of the nonlocal arrays
      into a single shared nonlocal array for the scope. A new compile phase,
      post analysis, is needed to assign nonlocal slot ids.
    * A nonlocal ref created during analysis needs to store a key that can be
      mapped to a slot id after amalgamation. This is a Ref_ID, possibly a
      bitfield containing a stack depth and slot index. Or an Atom.
    * As a special case, the nonlocal array for a module will use the same slot
      array as the field values.
  * Simplified Design: All let/module function definitions are analyzed as
    RFunctions. Nonlocal slot indexes are assigned during analysis, no need for
    a new compiler pass. If a let has function definitions, then an extra slot
    is added for the function nonlocals list. The slot contains a thunk for
    a list constructor, so it's lazy. In a module, the nonlocals are added
    directly to the module's slot array.
* lambda expressions: x->expr: stack of frames
  * lambda syntax: a->expr, (a,b)->expr, [a,b]->expr, {a,b}->expr
    * Currying: (a)(b)(c)->expr, like a->b->c->expr.
      This abbreviation hardly seems worthwhile, only shows value in
      the context `f(a)(b)=...` definition which is patterned after ML.
    * Chains: `f x->x+1` parses as `f(x->x+1)`.
      Not a high priority, as infix operators generally don't work in chains.
    * For now, the left arg of `->` is a primary, anything else is an error.
      `->` has a low precedence.
      `a b->...` is an error (no currying or chaining).
    * `x->expr`  Special case when there is exactly one argument,
      doesn't work with the Currying abbreviation.
    * Maybe `f(x,y)children=...` is legal; naked identifier legal as final
      curried argument; this abbreviates `f=(x,y)children->...`.
  * syntax: is lambda a chain or an expr?
    * how is this parsed: `a b->expr` ?
      * As `(a b)->expr`, like `a->b->expr`? Expr interpretation.
        Doesn't generalize to `a b c->expr`. Not good. The dual,
        `f a b=expr`, doesn't work.
      * As `a(b->expr)`? Chain interpretation. Consistent with the general
        chain syntax.
      * An error.
* 'curv <filename>' evaluates a script file.
* comments
* better error messages
* interactive curv: special variable `_` is last value. _1,_2, or $1,$2 ...

Refactor:
* analyze let expressions using Bindings_Analyzer

# CURV0 (expressions):
* bug: (-1)^-0.5 error message needs parens around '-1'
* simplify this code (narrowing a Value):
  > auto value = eval(*expr);
  > assert(value.is_ref());
  > Ref_Value& ref{value.get_ref_unsafe()};
  > Module& module{dynamic_cast<Module&>(ref)};
  > return Shared<Module>(&module);
* x^y: right associative, -x^y==-(x^y), x^-y==x^(-y)
  **: algolw, Python, Rexx, Simula, Cobol, Ruby, Tcl
  ^^: D
  ^: Eiffel, Icon, Mathematica, Maple, Matlab, Basic, Awk, Bc, Lua, Julia, R
* double-conversion based strtod
* lexer parses C++14 preprocessor numerals, Numeral::analyse parses legal
  numeral while building a buffer for strtod.
* builtin functions:
  * error reporting for bad arguments:
    * `arg_to_num(Value,char* param_name)`: throw error if not a number,
      otherwise return double. API is independent of Value refactoring.
* num: min[x,y], max[x,y], abs(x), sin(x), cos(x)
* string: $$ $" $id ${id} $(id) $+EE interpolation, strcat[s1,s2]
  scanner has in-string flag, parser resets/sets flag during $(expr).
* list: [a..b], concat[l1,l2]
* string: str@i, len(str) -- either ASCII-only, or proper Unicode
* records: defined r.id, merge[r1,r2]
* broader unit testing
* smart pointers, see docs/Shared_Ptr.md

# CURV1 (scripts, LLVM)
* script files, named functions (top level only for now).
* top level if, for, echo
* pattern matching: hypot[x,y] = sqrt(x^2+y^2).
* LLVM based evaluator. Compile Meaning to LLVM IR.
  Embed raw function pointers and Values in IR as bit patterns.
* Module values?
* Benchmarks

# CURV2 (nested scopes)
* nested functions, function literals, let() expressions.
* curried functions `dist(req)(pt) = ...;` or function literals
* script literals. At least {dist=..,bbox=..}
* stack trace: exceptions with precise location reporting
* refactor: String, List don't contain vtable or type field. Value contains a 3
  bit type field. Object is heavy weight, containing vtable and RTTI, since it
  is user extensible without modifying Value.
  Not sure about Function (3 variants: static, closure, object).
* printing a function? currently, <function>, even for builtins

# CURV3 (simple geometry)
* F-Rep: cube, sphere, affine transforms, boolean CSG
* implement 3d primitives in Curv using shape3d. (also is_shape)
  * this means: I'm implementing language features I need for defining geometry
* preview using ray marching--convert CSG tree to GLSL

# CURV4 (geometry export)
* use dual contouring instead of marching cubes for STL export
* does OpenCL speed this up?
* SVX export?

# Ongoing:
* more unit tests
* language reference manual

---------------------------------------------------------------------------
# Later:
better error messages, print line with caret:
  ERROR: the 'what()' error message
  file foo.scad, line 1[10-15], token foobar
    x = foobar();
        ^-----
And/or colour the bad text in red.

* Digit separator ' in numerals (like C++14).
* Support ' in identifiers (like Haskell).
* Support `foo` quoted identifiers.
  Compatibility with OpenSCAD identifiers, GUI interfaces that display
  parameter names as GUI labels, JSON object keys.
* Rewrite scanner in re2c. Then add UTF-8 support.
* Parser: consider Boost::Spirit (recursive descent) or Lemon (similar to
  Bison, thread safe, LALR(1)), or Bison with %pure-parser (thread safe).
  Is Lemon better than Bison? Lemon:
  * the tokenizer calls the parser (instead of vice versa)
  * terminals/non-terminals don't need to be declared. Terminal name has
    initial uppercase letter.
  * productions are less error prone. Instead of $1, $2 (requires counting),
    expr(A) ::= expr(B) PLUS expr(C). { A = B + C; }
  * no mid-actions, one action per rule.
  * no repetitions and optionals.
  * small footprint: small code with no dependencies
* localization
* curv::String refactor
  * String_Builder is rewritten to use a curv::String as the internal buffer
  * User defined string literal constructs a Shared<String>:
    underlying object is constructed at compile time with no mallocs,
    and without two copies of the string literal data.
* dtostr() refactor
  * put all 4 parameters into a DStyle struct.
* aux::Exception/curv::String refactor. Aux shouldn't depend on Curv.
  Original plan: to put String in Aux. But we'll see how Curv runtime evolves.

build system?
* Fast. Correct: auto track dependencies, rebuild on recipe change,
  delete stale objects. High level: standard abstractions that are portable
  across linux/osX/win. Cross project dependency discovery and configuration
  (see pkg-config). Packageable: libcurv can be packaged for Ubuntu; build tools
  are already in ubuntu repo.
* Should I use pkg-config for build-time dependency discovery and configuration?
  Do I install a *.pc file that records compiler and linker flags required to
  use the curv library? Can I build an Ubuntu package for libcurv?
* Google Bazel is fast, correct and high level. Still in beta. Missing some
  platform support, cross project dependencies. Wait for 1.0 in 2017.
* cmake is popular and well supported on multiple platforms.
  It has comprehensive portability abstractions.
  It is more correct than expected, given that it generates makefiles.
  Rebuilds on recipe change, tracks header dependencies. Doesn't delete stale
  products. (Has an experimental option to generate ninja files.)
* ninja is fast. It rebuilds if recipe changes. Header dependencies depend
  on the gcc feature to output dependencies (suboptimal, I'd rather it monitor
  which files are opened by build tools).
  Need a generator: ninja+cmake, ninja+gen, ninja+bash, ...
* tup is fast and correct. Tup determines hdr file dependencies automagically.
  Tup rebuilds if recipe has changed. Tup deletes build products that can
  no longer be generated. Tup has a high level command language (Lua).
  * Tup is not in the Ubuntu package universe. There are PPAs...
    Building from source isn't working...
  * Tup is deeply magical and edgy. It uses FUSE, performs kernel magic to
    overlay source and object directories, and makes tools confused
    about what their current directory actually is, causing some things to
    break. This can be overcome using the ^c chroot flag, which requires
    setuid root. FUSE probably no longer works in MacOS ElCapitan due to SIP.
  * So edgy it might discourage contributors. The automagic dependency tracking
    requires deep magic that is also flaky: fuse, chroot, dynamic lib injection.
    Eg, the ConEmu terminal emulator breaks DLL injection on Windows.
    Tup is reported to not work on FreeBSD (fuse works differently).
* redo. Simple, powerful, elegant. Also slow (as fast or slower than make).
* So, probably cmake+ninja.
