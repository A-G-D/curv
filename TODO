# 0.1: Minimum Viable Language
* len(module), module.[i]
* floor, ceil
* curv tool automatically locates std.curv
  * Automatically find the lib directory using <pathname of executable>/../lib?
    Port pn_progdir_x to boost::filesystem, put it in aux.
    This is compatible with installation in /usr/local, or with a binary tarball
    with bin and lib directories that is installed anywhere.

It's complicated...
* names: Phrase or Syntax?
* definitions are compiled into actions.
  * module (recursive scope): A definition may bind multiple names. Each such
    name has a slot containing the same Action thunk, which when evaluated
    updates all of the slots with their values.
  * qualified expr (sequential scope): each definition is compiled into an
    action that evaluates definientia and stores values in binding slots.
    At run time, execute the actions in order, then evaluate the expr.
  * record (no scope): compile definitions into actions, execute actions to
    initialize the record value. Records don't use slots, so we need a variant
    action type for updating record fields?
  * Definition::analyze(env) returns an action.
* `[x,y]=...` using above framework
* Extend `let` to support recursive functions.
* `expr where (defs)`, higher precedence than `let`, same semantics.
* (def1; def2; phrase) qualified expressions, sequentially scoped, see Loop.md
  * Each def compiles into an action which stores values in slots.
    Actions are executed in sequence, then expr is evaluated.
* echo, assert actions
  * assert(condition);     // generator, produces 0 values
  * assert(condition) expr // expression
  * (assert(condition); expr)  // see Loop.md; my current preference.
    * assert and echo are metafunctions. Builtin_Echo::call() returns an
      Echo_Expr, which is an effect/generator but not an expression.
      See also `each`, below.
* each <list> generator
  1. `each` is a builtin metafunction. Use a subclass of `Builtin`.
  2. `each` is a builtin generator function, so `each` is a value.
     Extend `Function` with a generate callback and add Call_Expr::generate().
  3. each(v) = for (i = v) i; Not hard: just add Call_Expr::generate().
  4. Now that I have sequence generators, `each` is not a high priority.
* Top level sequence generator. In curv tool, I type `1,2,3` at the prompt
  and it evaluates the sequence, without parens. `(1,2,3)` already works.
  * Semicolon has lower precedence than colon. (C has the same precedence.)
    Eg, so `a=1;a,a,a`.
  * A semicolon phrase is one or more comma phrases separated by semicolons.
    A comma phrase is zero or more rexprs separated by comma, with an optional
    trailing comma. Any emptiness in a semicolon phrase is represented by
    empty comma phrases. Extra semicolons `;;;` can be inserted into a
    semicolon phrase.
  * A comma phrase is a sequence of generators, executed in sequence.
  * `()` is technically an empty comma phrase. It is a special phrase:
    it's the empty generator and it is also the empty action.
  * A semicolon phrase is a qualified expression. The initial elements are
    definitions and actions, executed in sequence and creating a local
    sequential scope. The final element is an operation that is bound in
    the context of this scope. The 'result' of a semicolon phrase is the
    final element.
  * If the final element of a semicolon phrase is the empty action *epsilon*,
    then the entire phrase is an action, and this is the syntax we use for
    compound actions.
  * Meaning of a semicolon phrase in different contexts:
    * script: a module with recursive scoping.
    * {script} -- eg, {def1;def2;} a submodule literal, with recursive binding.
    * {def1,def2} -- a record literal with no scope
    * let(a=...;b=...)body -- local bindings with recursive scope, ML `let rec`
    * let(def1,def2)body -- local bindings with simultaneous scope, ML `let`
    * (q1;q2;...;qn;body) qualified phrase, sequential scoping
    * [q1;q2;...;qn;body] qualified generator, sequential scoping
    * so `;` separates definitions when they are in a recursive or sequential
      scope relationship, or `,` separates them when they can't see each other.

Other:
* Interactive curv: special variable `_` is last value. _1,_2, or $1,$2 ...
* Introduce submodules (with recursive functions)
* Shared constructor is protected. Add share<T>(T&) which aborts
  if use_count==0.
* Dynamically pluggable system abstraction. `System` class with system.echo(),
  system.warning(), system.error(), system.open(), system.namespace, etc.
  System object is refcounted, available during compile and run time.
  * Scanner::system_
  * Environ::system_. Just a single Environ object for the entire analysis
    phase, which points to a chain of Scope objects.
  * builtin_namespace(system) is a function that injects the system object into
    certain builtin functions, like `file`, avoids the cost of making system
    generally available in all evaluation contexts.
  * Do we need the system object to be a VM register? Maybe needed if
    system.error(Exception) is used to throw errors (so that debugger can be
    triggered on error). So then:
    * thread local variable
    * store in Frame--more efficient than passing as argument to eval()
      and Function::function_().
* interrupt.
  * Periodically test a thread local variable known to the curv core,
    curv::interrupt_requested. Throw an exception on interrupt, or call a System
    function to do this. I don't like thread local variables for stylistic
    reasons, but it seems legit to use one for IPC.
  * Periodically call a System function. Inside this function provided by
    the client is `if(thread_local_interrupt!=0)throw Interrupt()`.
    This keeps the core "pure" (free of thread-local variables) at the cost
    of a virtual function call at frequent intervals. The client will define
    the thread-local variable instead.
* still need this?
  - Environ must contain a Frame and a System. How?
    - Every Environ contains a copy of the Frame and System pointers.
    - A single mutable Environ object for the entire analysis phase,
      which points to a stack of Scope objects.
      Use RAII (in Scope class) to push/pop scopes.
      So Scope contains a reference to Environ.
      Environ::lookup calls Scope::lookup.
      More complex, but a cleaner data model, since it doesn't make it look
      like each Scope has an independent choice of Frame and System.

# CURV0 (expressions):
* bug: (-1)^-0.5 error message needs parens around '-1'
* simplify this code (narrowing a Value):
  > auto value = eval(*expr);
  > assert(value.is_ref());
  > Ref_Value& ref{value.get_ref_unsafe()};
  > Module& module{dynamic_cast<Module&>(ref)};
  > return Shared<Module>(&module);
* x^y: right associative, -x^y==-(x^y), x^-y==x^(-y)
  **: algolw, Python, Rexx, Simula, Cobol, Ruby, Tcl
  ^^: D
  ^: Eiffel, Icon, Mathematica, Maple, Matlab, Basic, Awk, Bc, Lua, Julia, R
* double-conversion based strtod
* lexer parses C++14 preprocessor numerals, Numeral::analyse parses legal
  numeral while building a buffer for strtod.
* builtin functions:
  * error reporting for bad arguments:
    * `arg_to_num(Value,char* param_name)`: throw error if not a number,
      otherwise return double. API is independent of Value refactoring.
* num: min[x,y], max[x,y], abs(x), sin(x), cos(x)
* string: $$ $" $id ${id} $(id) $+EE interpolation, strcat[s1,s2]
  scanner has in-string flag, parser resets/sets flag during $(expr).
* list: [a..b] or [a to b by step], concat[l1,l2]
* string: str@i, len(str) -- either ASCII-only, or proper Unicode
* records: defined r.id, merge[r1,r2]
* broader unit testing
* smart pointers, see docs/Shared_Ptr.md

# CURV1 (scripts, LLVM)
* script files, named functions (top level only for now).
* top level if, for, echo
* pattern matching: hypot[x,y] = sqrt(x^2+y^2).
* LLVM based evaluator. Compile Meaning to LLVM IR.
  Embed raw function pointers and Values in IR as bit patterns.
* Module values?
* Benchmarks

# CURV2 (nested scopes)
* nested functions, function literals, let() expressions.
* curried functions `dist(req)(pt) = ...;` or function literals
* script literals. At least {dist=..,bbox=..}
* stack trace: exceptions with precise location reporting
* refactor: String, List don't contain vtable or type field. Value contains a 3
  bit type field. Object is heavy weight, containing vtable and RTTI, since it
  is user extensible without modifying Value.
  Not sure about Function (3 variants: static, closure, object).
* printing a function? currently, <function>, even for builtins

# CURV3 (simple geometry)
* F-Rep: cube, sphere, affine transforms, boolean CSG
* implement 3d primitives in Curv using shape3d. (also is_shape)
  * this means: I'm implementing language features I need for defining geometry
* preview using ray marching--convert CSG tree to GLSL

# CURV4 (geometry export)
* use dual contouring instead of marching cubes for STL export
* does OpenCL speed this up?
* SVX export?

# Ongoing:
* more unit tests
* language reference manual

---------------------------------------------------------------------------
# Later:
better error messages, print line with caret:
  ERROR: the 'what()' error message
  file foo.scad, line 1[10-15], token foobar
    x = foobar();
        ^-----
And/or colour the bad text in red.

* Digit separator ' in numerals (like C++14).
* Support ' in identifiers (like Haskell).
* Support `foo` quoted identifiers.
  Compatibility with OpenSCAD identifiers, GUI interfaces that display
  parameter names as GUI labels, JSON object keys.
* Rewrite scanner in re2c. Then add UTF-8 support.
* Parser: consider Boost::Spirit (recursive descent) or Lemon (similar to
  Bison, thread safe, LALR(1)), or Bison with %pure-parser (thread safe).
  Is Lemon better than Bison? Lemon:
  * the tokenizer calls the parser (instead of vice versa)
  * terminals/non-terminals don't need to be declared. Terminal name has
    initial uppercase letter.
  * productions are less error prone. Instead of $1, $2 (requires counting),
    expr(A) ::= expr(B) PLUS expr(C). { A = B + C; }
  * no mid-actions, one action per rule.
  * no repetitions and optionals.
  * small footprint: small code with no dependencies
* localization
* curv::String refactor
  * String_Builder is rewritten to use a curv::String as the internal buffer
  * User defined string literal constructs a Shared<String>:
    underlying object is constructed at compile time with no mallocs,
    and without two copies of the string literal data.
* dtostr() refactor
  * put all 4 parameters into a DStyle struct.
* aux::Exception/curv::String refactor. Aux shouldn't depend on Curv.
  Original plan: to put String in Aux. But we'll see how Curv runtime evolves.

build system?
* Fast. Correct: auto track dependencies, rebuild on recipe change,
  delete stale objects. High level: standard abstractions that are portable
  across linux/osX/win. Cross project dependency discovery and configuration
  (see pkg-config). Packageable: libcurv can be packaged for Ubuntu; build tools
  are already in ubuntu repo.
* Should I use pkg-config for build-time dependency discovery and configuration?
  Do I install a *.pc file that records compiler and linker flags required to
  use the curv library? Can I build an Ubuntu package for libcurv?
* Google Bazel is fast, correct and high level. Still in beta. Missing some
  platform support, cross project dependencies. Wait for 1.0 in 2017.
* cmake is popular and well supported on multiple platforms.
  It has comprehensive portability abstractions.
  It is more correct than expected, given that it generates makefiles.
  Rebuilds on recipe change, tracks header dependencies. Doesn't delete stale
  products. (Has an experimental option to generate ninja files.)
* ninja is fast. It rebuilds if recipe changes. Header dependencies depend
  on the gcc feature to output dependencies (suboptimal, I'd rather it monitor
  which files are opened by build tools).
  Need a generator: ninja+cmake, ninja+gen, ninja+bash, ...
* tup is fast and correct. Tup determines hdr file dependencies automagically.
  Tup rebuilds if recipe has changed. Tup deletes build products that can
  no longer be generated. Tup has a high level command language (Lua).
  * Tup is not in the Ubuntu package universe. There are PPAs...
    Building from source isn't working...
  * Tup is deeply magical and edgy. It uses FUSE, performs kernel magic to
    overlay source and object directories, and makes tools confused
    about what their current directory actually is, causing some things to
    break. This can be overcome using the ^c chroot flag, which requires
    setuid root. FUSE probably no longer works in MacOS ElCapitan due to SIP.
  * So edgy it might discourage contributors. The automagic dependency tracking
    requires deep magic that is also flaky: fuse, chroot, dynamic lib injection.
    Eg, the ConEmu terminal emulator breaks DLL injection on Windows.
    Tup is reported to not work on FreeBSD (fuse works differently).
* redo. Simple, powerful, elegant. Also slow (as fast or slower than make).
* So, probably cmake+ninja.
