# Unicode
Curv currently restricts programs and strings to ASCII, since Unicode is too
complex for humans to understand and will create too many bugs.

In addition to the inhuman complexity, the Unicode standard keeps changing.
So the semantics of strings and identifiers changes as Curv is updated to
understand new Unicode versions. So a program that works under one version
of Unicode may break when Curv is updated. Or, a program that works in current
Curv may break if run by somebody running an old version of Curv.

But maybe there will be a requirement to support Unicode in the future.
How badly will I be screwed? What are the tradeoffs for different approaches
to handling Unicode?

I should make separate decisions about permitting Unicode in comments,
string values, and identifiers/field names.

## Symbols and Uninterpreted Strings
The new design (Symbols and Structures) proposes that strings are uninterpreted
blobs. You can construct a string using a string literal, and you can
concatenate a string. But you don't compare strings for equality or decompose
them into a sequence of characters. Under this design, the problems of
Unicode become more manageable.

Symbols can be compared for equality, but we will restrict what Unicode
strings are legal as Symbols, and we'll NFC normalize, to eliminate
problems.

## Unicode comments
This is relatively safe, since comments are (for now) uninterpreted.
If Curv gets "documentation comments", then issues may arise.

## Unicode Identifiers and Field Names
This is a minefield of buggy behaviour. We have to consider: when are two
identifiers considered equal? Directory record syntax adds to the problem.

Unicode recommends: "All user-level comparison should behave as if it
normalizes the input to NFC."

MacOS converts all filenames to NFD for storage in HFS+. So MacOS compares
filenames that are in normal form. Common Linux file systems know nothing
about Unicode, so filenames do not need to be legal UTF-8, and there is no
normalization. You can have two distinct files whose names compare equal if
normalized. In most cases, Linux filenames will be in NFC, since keyboards
and keyboard input processes generally create NFC (including on MacOS!).
The internet has also standardized on NFC, it's the most popular normalization
form.

If I support Unicode identifiers, then I *must* normalize to NFC before
comparing them. Otherwise there will be bugs. Directory syntax is one possible
source of bugs. On MacOS, I could get an NFD identifier via readdir and an
NFC identifier from the source code. I saw a bug like this on github.

BUT this creates another problem. The normalization algorithm changes from
one release of Unicode to the next: when new code points are assigned, they
may be subject to normalization, whereas unassigned code points are not changed
during normalization. And this means that identifier equality changes from
one Unicode release to the next, *if* identifiers are permitted to contain
unassigned code points. To fix this, unassigned code points are not permitted
in an identifier. Each release of Curv is tied to a specific Unicode version.
Updating to a new release of Curv may also update the Unicode version, and
add new code points that are permitted in identifiers.

For performance reasons, I should store identifiers in NFC form.
So curv::Symbol is NFC normalized. It's probably a good idea to ensure that
curv::String is NFC normalized as well.

In addition to normalization, there is also the problem of distinct identifiers
that look identical even when normalized. For example, "AB" (Latin script),
vs "ΑΒ" (Greek script), vs "АВ" (Cyrillic script). Until the security community
agrees on a standard solution, I can't do anything about this.

The safest policy is to only support ASCII identifiers and field names.
If I support Unicode strings, then Curv support for string field names,
as in R."funny business", conflicts with this.

I might want to support control characters (eg, newline) in strings.
I can't think of a reason to support them in identifiers.

## Unicode Stability and Versioning
The Unicode standard keeps changing, which affects the semantics of operations
on Unicode strings, which affects the meaning of Curv programs. Potentially,
a change to Unicode can cause a Curv boolean expression to change from true
to false. I don't want the meaning of a Curv program to depend on which version
of an external library dependency that you link with when you build Curv
from source.  So each commit in the Curv repository is tied to a specific
version of Unicode. Curv will contain a copy of the Unicode character property
table, and that will be periodically updated to new Unicode versions.

Some Unicode character properties are immutable: they can't change, even to
fix bugs. But this immutability guarantee only applies to properties of assigned
code points. So if I want Curv semantics to only rely on immutable properties,
then unassigned code points must be illegal in Curv strings.

A "fixed property" can change, but only to fix obvious or clerical errors.
Should I allow Curv string semantics to depend on fixed properties?

## Safe Unicode Subset
Unicode is a shitstorm of endless complexity and buggy behaviour.
Restricting the set of code points that we support in strings and identifiers
can help avoid some bugs.

Put it this way: right now, my safe Unicode subset is a subset of ASCII.
Here, I'm proposing to vastly expand the safe subset by using a black list
instead of a white list. The safe subset could be expanded in the future,
without causing problems, but contracting it is not backward compatible.
So it pays to be strict and just ban all dubious code points.

* Only legal code points may be used.
  Any integer outside the range 0 .. 0x10FFFF is not a legal code point.
* Surrogates are integers in the range 0xD800 to 0xDFFF. They are not legal
  code points (they cannot be legally encoded as code points in UTF-8).
  Some sources refer to them as "surrogate code points". Not supported.
* There are 66 "non-character code points" which are guaranteed to never be
  used for a character. These are not supported.
* Private use code points are not supported. Maybe they could be subject
  to private normalization, leading to problems in the definition of string
  or identifier equality. Whatever: if I don't know what these code points
  mean, then they are not supported.
* Unassigned code points are not supported. See reasons in **Identifiers**.
* The code point 0 (NUL) is not supported. Curv uses C library code and
  NUL terminated strings in its implementation, so this avoids buggy behaviour.
* "These code points do not have a Name (na=""): Controls (category Cc),
  Private use (Co), Surrogate (Cs), Non-characters (Cn) and Reserved (Cn)."
  I banned all of these scalars above, except for Controls, of which there are
  65 (all in ASCII and Latin-1).

Incorporate this information:
https://en.wikipedia.org/wiki/Valid_characters_in_XML

What do these options mean in the utf8proc library?
 - strip "ignorable" (@ref UTF8PROC_IGNORE) characters,
   control characters (@ref UTF8PROC_STRIPCC),
   or combining characters such as accents (@ref UTF8PROC_STRIPMARK)

More research needs to be done...

See String Concatenation for a more radical proposal: safe characters
are restricted to EGCs that are represented by a single code point in NFC.
Combining characters, jamo, etc are forbidden. With this restriction,
* A string can be considered a list of characters.
* count (strcat(s1,s2)) == count(s1)+count(s2)
* The concept of a character literal becomes more meaningful.
* With no combining characters, C-like character literals 'a' and string
  literals "abc" will be rendered correctly, because the first character will
  not try to merge with the preceding quote character into a single glyph.

## Unicode Strings
An excellent analysis of unicode string length:
https://hsivonen.fi/string-length/

A string is a sequence of characters, but what is a character in Unicode?
 1. Character boundaries.
    * In Swift, a character is an "extended grapheme cluster". Eg, an emoji
      flag character consists of two code points, but counts as one character.
      The 'string.count' method iterates over the string.
      The Unicode standard says that a "user perceived character"
      is most closely approximated by an "extended grapheme cluster".
      (Any closer approximation is locale-dependent.)
      http://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries
      Regular expressions should ideally use EGCs as character boundaries.
      If I can find an open source library that implements these semantics,
      may I should use it.
       * https://github.com/michaelnmmeyer/grapheme
         It uses Ragel, which is similar to re2c.
         Ragel is in the Ubuntu Universe.
    * But, there is no stability guarantee for EGC boundaries, unlike the
      stability guarantee for normalization. For example, the definition of
      EGC boundaries for emoji flag characters (two adjacent code points)
      changed, after emoji flag characters had already been released. It was
      a bug fix, but it impacted the Swift language by changing the meaning
      of the string length and index operators.
    * Given the instability of EGCs, are they worth it? Do they provide any
      worthwhile guarantees? English speakers might think that an EGC
      corresponds to a glyph, or to the unit of character selection,
      but I don't think that's true in general. In some Asian scripts,
      a single glyph may be constructed from multiple EGCs.
      https://gankra.github.io/blah/text-hates-you/
    * Or, a character is a code point. This has many technical advantages.
      * The Unicode standard is unstable. EGC boundaries change from one
        release to the next. Eg, when new compound emoji classes are added.
      * The definition of a code point should now be stable. It changed once
        when Unicode expanded beyond 16 bits, which screwed over languages
        that standardized on 16 bit code points, like Java and JavaScript.
        But I don't expect that to happen again, so code points are safe
        to rely on.
      * It is simpler to implement. Way too much complexity otherwise.
    * I suppose that users may eventually want to split strings into EGCs.
      Since the results depend on Unicode version, the API should be in an
      external library that is versioned.
 2. When are two strings equal?
    If I don't normalize strings on construction, and I don't normalize on
    comparison, then I will have bugs. Bugs will occur with "directory syntax",
    where a field name in a Dir_Record will compare unequal to a field name
    supplied by user code. MacOS converts all filenames to NFD for storage in
    HFS+. MacOS keyboard shortcuts generate NFC. Many other external processes
    convert strings to NFC.

    * Maybe, if they represent the same sequence of code points.
      But this means, two strings that normalize to the same thing may be
      unequal.
    * Unicode recommends: "All user-level comparison should behave as if it
      normalizes the input to NFC." But this means, two strings with a different
      number of code points can be equal. If `count` counts the number of code
      points, not the number of EGCs, then this is visible.
    If two grapheme clusters are guaranteed (by the Unicode standard)
    to have the same printed representation, then they compare equal?
    * If two code point sequences are canonically equivalent, then they should
      test equal, independent of locale.
      https://en.wikipedia.org/wiki/Unicode_equivalence
    * Is there open source for comparing two unicode strings and determining
      canonical equivalence?
    * Is it easier or harder to pre-normalize all Curv strings and then
      test byte-level equality? If I support Unicode in symbols, then those
      should probably be normalized, for efficiency reasons.
      Normal forms are not closed under concatenation. strcat two normalized
      strings and you can get an unnormalized string.
    * Does canonical equivalence change from one Unicode release to the next?
      There are stabilility guarantees, weakened by weasel words.

It is impossible to view a Unicode string as a list of characters, in the
same way we think of an ASCII string as a list of characters, because none of
the algebraic invariants associated with that mental model actually hold.
* Maybe we treat strings as atomic. You can't count the characters
  in a string, or request the n'th character, because that is ill defined.
  Basically, the entire concept of a character is ill defined in Unicode.
* Strings are internally represented as lists of code points in NFC form.
  You can convert between strings and lists of code points.

We could treat Unicode strings as lists of code points. Then we get a
simple implementation and the expected algebraic invariants. But a character
is a sequence of one or more code points. And we run into problems if we
convert these strings into identifiers, IE field names in a record.
* The weirdness goes away if we require field names to be ASCII.
* Maybe we normalize strings to NFC before conversion to field names.
  Then two unequal strings map to the same field name.

## String Concatenation.
Suppose S1 and S2 are strings.
We can't guarantee that count(strcat(S1,S2)) == count S1 + count S2.
* Suppose that a character is defined to be an EGC.
  Then string concatenation may combine code points at the end of the first
  string with code points at the start of the second string to create a
  single EGC.
* Suppose that a character is defined to be a code point. And suppose that
  strings are stored in NFC, to speed up equality tests, etc.
  String concatenation is not closed under normalization, so we need to
  renormalize, and that may change the number of code points.

Maybe we can fix this problem by restricting the set of legal Unicode
characters, and/or by disallowing "defective" code point sequences at
the start and end of a string? Man, that is a big can of worms.
* Does this restriction cause a problem for users?
* Does the Unicode standard support an algorithm for detecting these
  defective strings? If not, then the idea isn't feasible.

I think there is a zero width unicode character that prevents the left
and the right neighbours from coalescing into a single EGC. The string
concat operator could emit this ZW code as necessary to preserve the
identity of EGCs under concatenation. Then the code is not counted as
part of the length of a string.

Maybe we define a character as a code point, and we don't store strings
in normalized form. Then, if we want fast string equality, we need an
algorithm that compares two strings and normalizes on the fly. That's
very complicated, and I haven't found an open source implemention yet.
(I can imagine compiling the Unicode character property table into a
state machine that does this, and the state machine could be compiled into
efficient machine code. You rerun the metaprogram each time the standard
changes. I don't think you want to code this by hand.)

Maybe we restrict our safe subset of Unicode to EGCs that are represented by
a single code point in NFC. Combining characters, jamo, etc, are forbidden.
Then the distinction between code point and character doesn't matter: they
are the same thing. The only time we normalize strings is when receiving
text from the operating system: text files, command line options, filenames
from readdir(). Curv string operators that construct new strings don't
normalize their output. `decode` just rejects code points it doesn't like.
* PRO: Leads to simpler code that is less likely to conceal weird bugs.
  Such as: the bug that crashed iPhones when you texted a poisoned Unicode
  sequence.
* CON: Users will be pissed if they can't use the entire Unicode repertoire.
  No emoji flags or skin tone modifiers.
  No jamo means no archaic Korean characters, only modern ones.
  No combining diacritics means no archaic accented latin characters.

If I further restrict "safe" Unicode to the BMP, then I can lock down the
Unicode property table. I can represent code points in 16 bits (UTF-16 subset),
and have constant-time string indexing. I can avoid interoperability problems
with old Unicode software. I could provide a font that contains all of the
supported characters, and a text primitive that renders all legal Curv strings,
and that functionality would be locked down and would not need updating.
* No emojis at all.

A possible advantage of no emojis is that the Curv 'text' primitive
can define a character as pure geometry with no colour field. It's simpler.

If there is a one-to-one correspondence between code points and characters
and glyphs, that is also simpler. Text rendering becomes simple, and the
user API for measuring and controlling text rendering is simplified.

## Non-defective Unicode Strings?
https://en.wikipedia.org/wiki/Unicode_equivalence

This Wikipedia page tries to enumerate the situations where string concatenation
is closed under normalization. I had some brief hope that I could require
Curv strings to be "non-defective", and thus have well behaved string
concatenation. Alas, I now suspect that the requirements for well behaved
string concatenation could change with new releases of Unicode.
For example, I think that the addition of emoji flag sequences added a new kind
of defective sequence. I don't want to be in charge of maintaining the code
that checks for "defective strings", updating it at each Unicode release.

For non-defective Unicode strings not starting with a Hangul vowel or trailing
conjoining jamo, the normal forms are closed under string concatenation:
the concatenation of two well-formed Unicode strings in the same normal form
will itself be in that normal form. This is because a non-defective Unicode
string never begins with a combining character[3] and the only two kinds
of non-combining characters, which may interact with preceding characters
during composition, are Korean vowel conjoining jamos and trailing conjoining
jamos.[citation needed] Note, however, that this is not true of arbitrary
strings: accents are canonically ordered and may be rearranged around the
point where the strings are joined (for example, concatenating "a◌̂"
and "◌̣" will produce "a◌̂◌̣", but the correct normalization is
"a◌̣ ◌̂").[4]

However, they are not injective (they map different original glyphs and
sequences to the same normalized sequence) and thus also not bijective
(can't be restored). For example, the distinct Unicode strings "U+212B"
(the angstrom sign "Å") and "U+00C5" (the Swedish letter "Å") are both
expanded by NFD (or NFKD) into the sequence "U+0041 U+030A" (Latin letter
"A" and combining ring above "°") which is then reduced by NFC (or NFKC)
to "U+00C5" (the Swedish letter "Å").

A single character (other than a Hangul syllable block) that will get replaced
by another under normalization can be identified in the Unicode tables for
having a non-empty compatibility field but lacking a compatibility tag.

## Perl6 String Representation
Perl6 allows efficient indexing of EGSs using a fancy string representation.
https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc

A string has 3 forms:
 * 32-bit buffer of graphemes (Unicode codepoints or synthetic codepoints)
 * 8-bit buffer of codepoints that all fall in the ASCII range
 * Buffer of strands: list of non-strand strings.
   No recursive strands: this simplifies iteration.
   Created by: concatenation, substring ops & string repeat op.

A Strand is: a general string reference, start + end indexes, repetition count.

When two flat strings are concatenated, a Strand with references to
both string a and string b is created. If string a and string b were strands
themselves, the references of string a and references of string b are copied
one after another into the Strand.

A Grapheme is a 32 bit signed int. A negative grapheme (a Synthetic) denotes
2 or more code points.

All strings are stored normalized using NFC. Graphemes are segmented
(which codepoints are a part of which graphemes) following Unicode’s Text
Segmentation rules for Grapheme Clusters Technical Report 29.

Synthetics are graphemes which contain multiple codepoints. In MoarVM these
are stored and accessed using a trie, while the actual data itself stores
the base character seprately and then the combiners are stored in an array.

A Synthetic’s codepoints are stored in a single array, with the base
character pointed to by storing the location of its index in the array. The
base character may not be the first codepoint, due to combining characters
with the Unicode Prepend property.

NOTES:
This may be buggy. Does Perl6 implement the weird semantics of Unicode
string concatenation? With this representation, correctness would be tricky.

## Swift String Representation
A Character is an Extended Grapheme Cluster. The representation is 64
bits, which either contains 4 inline BMP code points (in 63 bits) plus a 1 bit
flag meaning small or large. The large representation is a pointer.

A Character can be converted into an array of code points.

A character literal is no longer restricted to containing a single EGC,
due to bugs.swift.org/browse/SR-4955, where a character literal that
contained two japanese flags (interpreted as a single EGC in Unicode 9)
turned into an error under Unicode 10. Although, I would consider allowing
an upgrade to a new Unicode standard to invalidate old programs, because
really there is no choice. If the Unicode table changes, then some old
programs may break, QED.

String equality works as if both strings are in canonical form.
However, strings preserve whatever sequence of code units that you provide
as input. And yes, they did it: they implemented the algorithm for comparing
two strings in non-canonical form. It is infinitely complex. A string is
represented as either an ASCII string or a general Unicode string, and there
is fast-path code to deal with the ASCII case. Right now, the general case
is represented as UTF-16, but they are moving this to UTF-8.
A String value is 64 bits; a small string can represent up to 7 ASCII
characters inline; a large string stores a pointer in the same space.

The Swift implementation is the right way to do Unicode strings.
Better than anything else I've found in an open source language, anyway.

## Strings are Lists of Characters
I am considering a change so that a string is a list of 'string elements'
(aka characters?) I'll need a suitable character data type.

We need to support list semantics:
* count(append[l1,l2]) == count(l1) + count(l2)
* You can amend a list, replacing one of the elements, without disturbing the
  other elements or changing the count.
* Two lists are equal if they have the same count and corresponding elements
  are equal.

To satisfy these requirements,
* Use Minimal Safe Unicode, where a code point is an EGC is a character.
  The requirements are met trivially, and strings are always in NFC.
* A string element is a code point, all graphical characters are supported.
  Strings are arbitrary lists of code points, not guaranteed to be NFC.
  To support Unicode compatible string equality,
  * The generic equality operator has special rules for contiguous subsequences
    of code points within a list.
  * The generic equality operator just implements simple list equality, and
    string equality is a separate operation.

In Kona, there is only one list concept: a list of values. Internally, there
are efficient representations for uniform lists of booleans, integers, etc.
In Julia, another dynamically typed language, list values have an associated
element type (which can be Any), and this is exposed in the language semantics.

## Simple Proposal:
This version has disappointing limitations, but the code complexity
is manageable.
* We only support a "safe" subset of Unicode. Only standalone code points
  that represent a single EGC in NFC. No compound EGCs. No astral plane.
* Internal representation is UTF-8. No need for normalization: once we reject
  unsupported code points, the result is always NFC.
* String comparison is just memcmp. Record field names use the same
  representation as strings, so we have fast identifier comparison.
* `count str` is the number of code points, and also the number of characters.
  `str[i]` is the i'th character, etc. These operations are O(N), not O(1).

## Proposal:
* A string is represented internally as UTF-8 and NFC,
  using only code points from a Mostly-Safe Unicode Subset that includes all
  graphic characters.
* String comparison is just memcmp. Record field names use the same
  representation as strings, and so have a fast compare operator.
* From a user perspective, a string is a list of code points in NFC.
  The core language has no concept of characters or EGCs.
* We need to perform NFC normalization when a curv::String is constructed,
  and after construction it must be immutable.
  * Amending a string, by changing a code point, may trigger renormalization
    and the number of code points may change. That's awkward.
* So, `count str` is the number of code points, `str[i]` is the i'th code point.
  These operations are O(N), not O(1).

## A UTF-8 String Library
A small, clean, maintained C library for UTF-8 strings.
Has clear licensing. Updated for each Unicode release.
https://github.com/JuliaStrings/utf8proc

When you normalize a string, it mallocs a new buffer for the result,
which conflicts with the curv::String representation.
(NFC normalization can expand a string, eg by decomposing certain ligatures.

## References
No matter how messed up I think Unicode is, the reality is always worse.
https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/
