Break free of JSON and support a more expressive system of fundamental types.
* Maps (aka Dictionaries)
* Sets (maybe)
* Symbols (maybe)
* Some data types have associated literal patterns.
* New function equality.
* Quoted identifiers.

Quoted Identifiers.
-------------------
'hello world' is a quoted identifier. Use cases:
* Permit embedded spaces and punctuation characters in identifiers.
  Useful if identifiers are shown in the GUI, via parametric records.
* Permit reserved words like 'if' and '_' to be identifiers.
* In Cue, within a scoped record literal,
  a field named _foo is private, but '_foo' is public. Maybe?

There is no reason to support control characters in quoted identifiers,
only printable ASCII (32-126) for now.

Within a string,
    $'foo'
may be an abbreviation of
    ${'foo'}

Escape sequences? Note they have to be resolved prior to semantic analysis.
For now, the only character that would need to be escaped is `'`. Later,
if Unicode is supported, we might want numeric code-point escape sequences.
Use case: include Unicode characters in the GUI, but the source code
remains ASCII for ease of editing. Eg, '$[0x263A]'.
Ideas:
* Initially, support two escape sequences: `$-` and `$.`.
  If Unicode happens, add numeric code-point escapes.
  (I note an unexpected advantage of `$-` over a more C-like `$'`:
  the latter conflicts with the meaning of `$'` in a string literal.)
* `''` could be an escape for `'`, following the original syntax for
  Curv string literals, but I abandoned that syntax style.

I'm not sure about interpolating named characters, because how do you
include a library of character names prior to semantic analysis?

Symbols.
--------
* Symbols are abstract values, distinguished only by their name.
  They only support equality and conversion to and from strings.
  #foo is a symbol; it prints as #foo; it is only equal to itself.
* #'hello world' is a symbol with nonstandard name. Used with dropdown_menu
  proposal: `dropdown_menu[#'Value Noise', #'Fractal Noise']`.
  (Note, not #"hello world" as that conflicts with the proposal for adding
   Swift5 string literal syntax.)
* Define true=#true, false=#false, null=#null.
* #foo is a pattern.
* Thus, #true and #false are the literal pattern syntax for booleans.
  (An alternative is for true and false to be keywords.)
* Record fields are symbols internally.
* `is_symbol x`

Variant Values (and Variant Types)
----------------------------------
Variant types are a feature of statically typed languages.
They have many names: discriminated unions, enums, variants and algebraic data types.
The core idea is that a variant type consists of a fixed set of alternatives.
Each alternative has a name, and optionally a value.
An enum type is one in which all of the alternatives are just names.

A variant (or variant value) is an instance of a variant type.
To construct a variant, you need to specify the name of one of the alternatives,
plus a value, if that alternative carries a value.

In Curv, we don't have explicitly declared variant types. Instead, variants
can be constructed directly. A variant with just a name is a symbol.
A variant with a name and a value is a record with a single field.
    #niladic
    {monadic: a}
    {dyadic: (a,b)}

A variant value is queried using pattern matching:
    match [
      #niladic -> [],
      {monadic: a} -> [a],
      {dyadic: (a,b)} -> [a,b],
    ]

I'd like to define a picker that takes a variant type as an argument.
It displays a drop-down menu for the tag, plus additional pickers for
data associated with the current tag value.
    variant_picker [ alternative, ... ]
Each alternative is either a symbol, or {tag: {record of pickers}},
or {tag: [alternative, ...]} if we want an alternate form of nesting.
The parameter that is bound to a variant_picker has a variant value that
must be queried using `match`.

Variant values are abstract: you use pattern matching to query the value.
There is another kind of tagged value which preserves the operations
on the value (eg, Cell tagged values, or Curv terms).

Variant Terms
-------------
Variant data types introduce a different way to model blending kernels:
    smooth 0.5 .union [a,b]
 vs smooth 0.5 {union: [a,b]}

We could unify these approaches by extending `record!` to take a variant
value as argument, like so:
    record!#foo    => record.foo
    record!{bar:a} => record.bar a

We could also do this in reverse: a piecewise function over variant values
could export field names for each tag. So,
    pfun #foo  => pfun.foo => pfun!#foo
    pfun {a:b} => pfun.a b => pfun!{a:b}

Also the cube:
    cube .mitred 5
    cube #mitred 5   or cube {mitred: 5}
If I want `cube` to be parametric, and the parameters include the exact/mitred
alternatives, what's the best way to organize the parameters,
and what do cube terms look like?

In a GUI, it might be good to present the cube parameters like this:
    size :: scale_picker = 2;
    field :: variant_picker[#exact, #mitred] = #exact;
    blend :: blend_picker = sharp;

Number patterns.
----------------
* <numeral> is a pattern.
* +<numeral> and -<numeral> are patterns.
* There needs to be a numeral for infinity, that isn't an identifier.
  * `0inf` -- consistent with the `0x` prefix. It's obviously a numeral.
    It is novel syntax, though.
  * `inf` is a keyword. Not obvious this is a keyword; not obvious that when
    used as a pattern, that it doesn't define a variable binding.
  * `#inf` is not preferred, I don't want infinity to be a symbol.
    I don't want is_symbol(inf) to be true. Why?
    * It would complicate the implementation of the Symbol type in SubCurv.
      Now we have one specific numeric value that belongs to the Symbol type,
      it complicates static type checking.
  * Most languages drop the ball, don't support a way to write infinity.
    Javascript has `Infinity`, Scheme has `+inf.0`, no standard.
    The corollary is that people mostly won't care what I choose.

String patterns.
----------------
We don't need string patterns. You shouldn't compare or match strings in Curv.
Use symbols instead.

Lists.
------
No change.

Sets.
-----
* The Python syntax {1,2,3} is great, but {} now denotes both an empty set
  and an empty record. In Python, {} is an empty dictionary,
  and set([]) is the empty set. In SETL, a map is a set of ordered pairs:
  this is consistent with the {...} syntax for sets. But how do I determine
  that a given set of ordered pairs is actually a map without copying the set
  into a hash table? That seems expensive.
* Thus, Set is disjoint from Record and Map.
* Syntax: #[1, 2, 3]
* A set pattern is a set literal containing constant patterns.
  You can't bind variables.

Function Equality.
------------------
Functions don't support proper equality. Semantics that I want:
1. You can't put a Function value in a Set or use it as a Map key (error).
   * Thus, you can't put Shapes into a Set, unless they are Terms.
2. If you match a Function value against a non-Function pattern,
   the match fails (no error).
3. As a corollary, for the proposed `==x` pattern,
   * Error if `x` is a function.
   * No match if the value being matched is a function.
4. Non-function predicates return false for function arguments.
   Eg, `is_bool(max) == false`.

What are the semantics of `a==b` for functions?
a. The current implementation is: all functions are equal.
   It's an equivalence relation. It seems inconsistent with the above semantics.
b. If a function appears as either argument of `==`, then an error.
   This seems consistent with the idea that Function is not an equality type,
   and it seems consistent with the equivalence relation axioms.
   However, suppose we define
      is_bool a = a==#true || a==#false;
   Then `is_bool(max)` is an error, contrary to the requirements.
c. If two functions are compared, an error. If a function and non-function
   are compared, false. This is compatible with the `is_bool` definition,
   and seems the most consistent with the required semantics above.
   It seems to violate the `a==a` axiom of an equivalence relation.
   (It is impossible to implement a==a for functions, if we want operational
   or mathematical equivalence.)

We will use 'c'.

ALTERNATIVELY, maybe we should support intensional function equality.

Idris allows you to prove that two functions are equivalent,
based on them having the same ASTs and non-local bindings, possibly differing
via eta- and beta-conversion. This is intensional, not extensional equality.
Shouldn't Curv be at least as expressive?

The Curv optimizer will perform CSE elimination based on some notion of
expression equivalence. The optimizer will also perform partial evaluation, and
that at least will be visible in the semantics (via which programs are in
SubCurv). If CSE somehow adds new opportunities for partial evaluation, then
CSE's rules of expression equivalence will become visible in the semantics.
Here's an example of how that could happen:
    if (expr1 == expr2) E1 else E2
If expr1 and expr2 are equivalent at compile time, replace this expression with
E1 (and it doesn't matter if E2 is in SubCurv). If expression equivalence
is in the semantics, then use it for function equality?

Records vs Maps.
----------------
Is a Record just a Map where the keys are Symbols?
* It seems elegant & mathematical. SETL and Cell work this way.
  Consider a map M where all the keys are symbols, and an isomorphic record R.
  What's the difference?
* Ah, but records and maps are used differently.
  A record type is a set of name/type pairs, which constitute an API.
  * Records are indexed using dot notation: R.F
  * A record with a `call` field is callable; that's part of its API.
  * `record.key := foo` reports an error if #key doesn't exist,
    while `map[#key] := foo` creates a binding for #key if it doesn't exist.
  A map type is Map(KeyType,ValueType).
  * In popular dynamic languages, maps are indexed using the same syntax
    as arrays: M[K]. A map with integer keys is like a sparse array.
So Records are not Maps.

Maps.
-----
* #{ key1 => value1, ... }
  Like Ruby syntax but with a #, so that #{} is the empty map.
* M[K] looks up key K in map M, returns the corresponding value.
* Each binding in a map pattern is `expr => pattern`. All of the keys
  in the map value being matched must match one of the keys in the bindings.

Records.
--------
Internally, field names are represented by Symbols.
* `fields aRecord` returns a Set of Symbols.

Right now, we overload the `:` statement as <string>:<expr> and <ident>:<expr>.
New design, we use
    <symbol-expr> => <expr>
to generate a field from a symbol expression, and use
    <pattern> : <expr>
to generate fields by binding against a pattern.
This frees up string literals to be used as literal patterns.
* We also need quoted identifiers, to satisfy the use case of non-standard
  identifiers as field names.
* The need for => to construct fields with computed names is questionable,
  once maps are added.

Indexing Operations.
--------------------
S!I
* list!integer | list![ix0,ix1,...]
* string!integer | string![ix0,ix1,...]
* record!symbol
  * vectorized indexing for records? Wait for more requirements.
    Maybe: record!set_of_symbols -> record
    Maybe: record!list_of_symbols -> list_of_values
* map!key
  * vectorized indexing is not possible: key can be a list or set.
* Motivations for this unusual syntax:
  * It's consistent across list, string, record and map.
    (Record doesn't support R[i] for indexing.)
    Might be useful for polymorphic operations like `fetch` or lenses?
  * Also, the `S!` syntax is potentially useful.
  * Unlike `fetch[i]S`, we can use `S!i` as a locative: `S!i := val`.

S!
Here ! is a postfix operator converting S to a function of an index value.

S[i,j,k,...]
* list[i,j,k] indexes a multidimensional array, can extract n-d slices.
* string[i] is same as string!i, map[k] is same as map!k.
  The reason for this redundancy is that this is the conventional syntax.

fetch [i1,i2,i3] S      or S >> fetch path
amend path newvalue S   or S >> amend path newvalue

Note on currying:
* path must be a separate argument so that fetch and amend can be used
  to construct lenses (see below).
* S must be a separate argument so that amend can be part of a structure
  pipeline.

Do fetch and amend support multi-d array slicing?

The old record indexing syntax is R."string-ctor", eg R."$a".
This syntax needs to go away.
But, R.(#foo) or R.(a) seems like a possible replacement.

Lenses.
-------
Simple Lens.
A Lens is a getter and a setter, bound together in a single data structure.
Like a combination of `fetch path` and `amend path` in Curv.
    data Lens struc item 
      = Lens { get :: struc -> item
             , set :: item -> struc -> struc
             }
or,
    lens path = {get: fetch path, set: amend path};
    l = lens mypath; l.get, l.set

Lens Composition.
    -- | Lens composition
    (>-) :: Lens a b -> Lens b c -> Lens a c
    la >- lb = Lens (get lb . get la) $ \part whole ->
      set la (set lb part (get la whole)) whole
semantics:
* Get the original middle part from the original big part
* Update the middle part with the new little part
* Update the big part with the new middle part
In Curv:
    lcomp(lens1,lens2) =
      { get:
      , set:
      };

Generalized Lenses.
https://www.schoolofhaskell.com/user/tel/lenses-from-scratch
