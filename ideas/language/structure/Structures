Break free of JSON and support a more expressive system of fundamental types.
* Maps (aka Dictionaries)
* Sets (maybe)
* Symbols (maybe)
* Some data types have associated literal patterns.
* New function equality.
* Quoted identifiers.

Quoted Identifiers.
-------------------
'hello world' is a quoted identifier. Use cases:
* Permit embedded spaces and punctuation characters in identifiers.
  Useful if identifiers are shown in the GUI, via parametric records.
* Permit reserved words like 'if' and '_' to be identifiers.
* In Cue, within a scoped record literal,
  a field named _foo is private, but '_foo' is public. Maybe?

There is no reason to support control characters in quoted identifiers,
only printable ASCII (32-126) for now.

Within a string,
    $'foo'
may be an abbreviation of
    ${'foo'}

Escape sequences? Note they have to be resolved prior to semantic analysis.
For now, the only character that would need to be escaped is `'`. Later,
if Unicode is supported, we might want numeric code-point escape sequences.
Use case: include Unicode characters in the GUI, but the source code
remains ASCII for ease of editing. Eg, '$[0x263A]'.
Ideas:
* Initially, support two escape sequences: `$-` and `$.`.
  If Unicode happens, add numeric code-point escapes.
  (I note an unexpected advantage of `$-` over a more C-like `$'`:
  the latter conflicts with the meaning of `$'` in a string literal.)
* `''` could be an escape for `'`, following the original syntax for
  Curv string literals, but I abandoned that syntax style.

I'm not sure about interpolating named characters, because how do you
include a library of character names prior to semantic analysis?

Symbols.
--------
* Symbols are abstract values, distinguished only by their name.
  They only support equality and conversion to and from strings.
  #foo is a symbol; it prints as #foo; it is only equal to itself.
* #'hello world' is a symbol with nonstandard name. Used with dropdown_menu
  proposal: `dropdown_menu[#'Value Noise', #'Fractal Noise']`.
  (Note, not #"hello world" as that conflicts with the proposal for adding
   Swift5 string literal syntax.)
* Define true=#true, false=#false, null=#null.
* #foo is a pattern.
* Thus, #true and #false are the literal pattern syntax for booleans.
  (An alternative is for true and false to be keywords.)
* Record fields are symbols internally.
* `is_symbol x`

Variant Values (and Variant Types)
----------------------------------
Variant types are a feature of statically typed languages.
They have many names: discriminated unions, enums, variants and algebraic data types.
The core idea is that a variant type consists of a fixed set of alternatives.
Each alternative has a name, and optionally a value.
An enum type is one in which all of the alternatives are just names.

A variant (or variant value) is an instance of a variant type.
To construct a variant, you need to specify the name of one of the alternatives,
plus a value, if that alternative carries a value.

In Curv, we don't have explicitly declared variant types. Instead, variants
can be constructed directly. A variant with just a name is a symbol.
A variant with a name and a value is a record with a single field.
    #niladic
    {monadic: a}
    {dyadic: (a,b)}

A variant value is queried using pattern matching:
    match [
      #niladic -> [],
      {monadic: a} -> [a],
      {dyadic: (a,b)} -> [a,b],
    ]

I'd like to define a picker that takes a variant type as an argument.
It displays a drop-down menu for the tag, plus additional pickers for
data associated with the current tag value.
    variant_picker [ alternative, ... ]
Each alternative is either a symbol, or {tag: {record of pickers}},
or {tag: [alternative, ...]} if we want an alternate form of nesting.
The parameter that is bound to a variant_picker has a variant value that
must be queried using `match`.

Variant values are abstract: you use pattern matching to query the value.
There is another kind of tagged value which preserves the operations
on the value (eg, Cell tagged values, or Curv terms).

Variant Terms
-------------
Variant data types introduce a different way to model blending kernels:
    smooth 0.5 .union [a,b]
 vs smooth 0.5 {union: [a,b]}

We could unify these approaches by extending `record!` to take a variant
value as argument, like so:
    record!#foo    => record.foo
    record!{bar:a} => record.bar a
This would allow you to write
    smooth 0.5 !{union: [a,b]}
which would allow {union: [a,b]} to be passed as a parameter then given
to smooth 0.5.

We could also do this in reverse: a piecewise function over variant values
could export field names for each tag. So,
    pfun #foo  => pfun.foo => pfun!#foo
    pfun {a:b} => pfun.a b => pfun!{a:b}
This idea is inconsistent with callable records and Algebras. It is ambiguous
if the piecewise function has patterns matching both #foo and {foo: x} --
the first such pattern would match, I guess.

Also the cube:
    cube .mitred 5
    cube #mitred 5   or cube {mitred: 5}
If I want `cube` to be parametric, and the parameters include the exact/mitred
alternatives, what's the best way to organize the parameters,
and what do cube terms look like?

In a GUI, it might be good to present the cube parameters like this:
    size :: scale_picker = 2;
    field :: variant_picker[#exact, #mitred] = #exact;
    blend :: blend_picker = sharp;

Number patterns.
----------------
* <numeral> is a pattern.
* +<numeral> and -<numeral> are patterns.
* There needs to be a numeral for infinity, that isn't an identifier.
  * `0inf` -- consistent with the `0x` prefix. It's obviously a numeral.
    It is novel syntax, though.
  * `inf` is a keyword. Not obvious this is a keyword; not obvious that when
    used as a pattern, that it doesn't define a variable binding.
  * `#inf` is not preferred, I don't want infinity to be a symbol.
    I don't want is_symbol(inf) to be true. Why?
    * It would complicate the implementation of the Symbol type in SubCurv.
      Now we have one specific numeric value that belongs to the Symbol type,
      it complicates static type checking.
  * Most languages drop the ball, don't support a way to write infinity.
    Javascript has `Infinity`, Scheme has `+inf.0`, no standard.
    The corollary is that people mostly won't care what I choose.

String patterns.
----------------
We don't need string patterns. You shouldn't compare or match strings in Curv.
Use symbols instead.

Lists.
------
No change.

Sets.
-----
* The Python syntax {1,2,3} is great, but {} now denotes both an empty set
  and an empty record. In Python, {} is an empty dictionary,
  and set([]) is the empty set. In SETL, a map is a set of ordered pairs:
  this is consistent with the {...} syntax for sets. But how do I determine
  that a given set of ordered pairs is actually a map without copying the set
  into a hash table? That seems expensive.
* Thus, Set is disjoint from Record and Map.
* Syntax: #[1, 2, 3]
* A set pattern is a set literal containing constant patterns.
  You can't bind variables.

Function Equality.
------------------
Currently, functions don't support useful equality. All functions are equal.
This means, putting function values in Sets or using them in Map keys leads
to meaningless results.

It's impossible to support true extensional equality for function values.
What are the alternatives?
Basic requirements are:
* Equality is an equivalence relation.
* Function equality is referentially transparent. If a function expression is
  copied within the same scope, both copies of the expression return values
  that are equal.

 1. The Abstract Value proposal adds "nominative" equality to function values.
 2. Functions can't be compared for equality, so they can't be used
    in Sets or as Map keys. See below.
 3. Support "intensional" function equality. See below.

Functions are Incomparable
~~~~~~~~~~~~~~~~~~~~~~~~~~
Semantics that I want:
1. You can't put a Function value in a Set or use it as a Map key (error).
   * Thus, you can't put Shapes into a Set, unless they are Terms.
2. If you match a Function value against a non-Function pattern,
   the match fails (no error).
3. As a corollary, for the proposed `==x` pattern,
   * Error if `x` is a function.
   * No match if the value being matched is a function.
4. Non-function predicates return false for function arguments.
   Eg, `is_bool(max) == false`.

What are the semantics of `a==b` for functions?
a. The current implementation is: all functions are equal.
   It's an equivalence relation. It seems inconsistent with the above semantics.
b. If a function appears as either argument of `==`, then an error.
   This seems consistent with the idea that Function is not an equality type,
   and it seems consistent with the equivalence relation axioms.
   However, suppose we define
      is_bool a = a==#true || a==#false;
   Then `is_bool(max)` is an error, contrary to the requirements.
c. If two functions are compared, an error. If a function and non-function
   are compared, false. This is compatible with the `is_bool` definition,
   and seems the most consistent with the required semantics above.
   It seems to violate the `a==a` axiom of an equivalence relation.
   (It is impossible to implement a==a for functions, if we want operational
   or mathematical equivalence.)

We will use 'c'.

Intensional Function Equality
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ALTERNATIVELY, maybe we should support intensional function equality.

Idris allows you to prove that two functions are equivalent,
based on them having the same ASTs and non-local bindings, possibly differing
via eta- and beta-conversion. This is intensional, not extensional equality.
Shouldn't Curv be at least as expressive?

The Curv optimizer will perform CSE elimination based on some notion of
expression equivalence. The optimizer will also perform partial evaluation, and
that at least will be visible in the semantics (via which programs are in
SubCurv). If CSE somehow adds new opportunities for partial evaluation, then
CSE's rules of expression equivalence will become visible in the semantics.
Here's an example of how that could happen:
    if (expr1 == expr2) E1 else E2
If expr1 and expr2 are equivalent at compile time, replace this expression with
E1 (and it doesn't matter if E2 is in SubCurv). If expression equivalence
is in the semantics, then use it for function equality?

Records vs Maps.
----------------
Is a Record just a Map where the keys are Symbols?
* It seems elegant & mathematical. SETL and Cell work this way.
  Consider a map M where all the keys are symbols, and an isomorphic record R.
  What's the difference?
* Ah, but records and maps are used differently.
  A record type is a set of name/type pairs, which constitute an API.
  * Records are indexed using dot notation: R.F
  * A record with a `call` field is callable; that's part of its API.
  * `record.key := foo` reports an error if #key doesn't exist,
    while `map[#key] := foo` creates a binding for #key if it doesn't exist.
  A map type is Map(KeyType,ValueType).
  * In popular dynamic languages, maps are indexed using the same syntax
    as arrays: M[K]. A map with integer keys is like a sparse array.
So Records are not Maps.

Modules, Records, Dictionaries
------------------------------
* A Dictionary is a data structure mapping arbitrary key values to values.
* A Record is a Dictionary where the keys are all symbols.
* A Module is a record-like abstract value with 'under fields'.
  A Module represents an API, where each field is a distinct entry point into
  the API. Used to represent libraries and instances of abstract data types.
  * A field name beginning with `_` is an 'under field'.
    The semantics are similar to Python: this denotes a field that is
    considered 'internal' or 'private'. Within an API, perhaps this field
    is not intended for general use, and has an unstable interface.
    Or perhaps this field is metadata.
  * A field name beginning with `__` is a 'dunder field`.
    Curv provides a set of special field names that modify the semantics of
    the Curv language for this module, if they are defined. These are:
    * __call : defines the meaning of function call for this module.
    * __identity : the module is a named value (see Abstract Values).

Here are some things I can't decide:
* Is {a:1} a Record or a Module? This is the thing we pass as a function
  argument to implement named parameters.
* When you print a Module without an __identity field, do you see <module>,
  or do you all the fields?
* When you call a Module without a __call field, do you get an error, or
  do you get the same result as calling a Dictionary? Eg, M[#foo] is M.foo.
  Bad idea: adding a __call field to a module shouldn't break existing code.

Two Alternatives:
 1. Function-like Modules
    A module with no __identity is printed as <module>, and all such modules
    compare equal. Not too useful, so almost all modules have identity, and
    modules are constructed using @{fields}. {fields} is a record literal
    and {key1=>val1,...} is a dictionary.
    To deliberately construct a module with no __identity, use #null@@{fields}.
    Configuration is a record. `R.id := newval` is legal for records, supports
    the REPL render_opts variable. Module fields are not assignable, because
    fields become out of sync with __identity.
    In SubCurv, both records and modules potentially need mapping to structs.
 2. Record-like Modules
    {fields} is a module literal (which has no __identity unless it was added
    by one of the subphrases). @{fields} modifies a module or function to set
    __identity. A module with no __identity prints all the fields.
    #{fields} is a record, #{key1=>val1,...} is a dictionary. These are only
    used when you just want a key/value map, with all keys having the
    same status, not a record where each field is an API.
    In fact, we can amp up the distinction between a dictionary and module:
      D[key] indexes a dictionary.
      M.id   indexes a module.
        List.id can be a vectorization, == [List[0].id,...]
      #{key1=>val1,...} is a dictionary literal.
      {id1:val1,...} or {id1=val1,...} is a module literal.
    Configuration is a module.
    In SubCurv, only modules are mapped to structs.

Criteria for choosing between these alternatives:
* The algorithm for finding picker parameters in a term distinguishes between
  record and shape arguments. Both alternatives work. With record-like modules,
  make_shape yields a module with __identity, and is_shape tests for __identity.
* `render_opts.aa := 4`?
  * With function-like modules, field assignment is illegal, but render_opts
    is a record, and this works.
  * With record-like modules, field assignment is illegal if the module
    has __identity.
* Record-like seems simpler, more elegant. Only one kind of record-like data
  structure indexed by dot notation, instead of two.
* Syntax for indexing into a dictionary, record and module:
  * Dictionaries: D[key], but no broadcasting, since lists can be keys.
  * S!index where S is a string,list,module,record,dictionary.
    Full broadcasting for lists and modules.
    1-D broadcasting for strings.
    No broadcasting for dictionaries.
  * With Record-like modules, {a:1,b:2}![#a,#b] is supported.
  * In most functional languages, there is a terse syntax for constructing a
    function that selects a field from a record. You pass it as an argument
    to combinators like `map`. Eg, in Elm, `.foo X` is an alias for `X.foo`.
    Following Clojure, we can use `#foo X` as an alias for `X.foo`.

The 2 alternatives have equal expressive power. It's down to esthetics.
Decision: record-like modules.

Maps.
-----
* #{ key1 => value1, ... }
  Like Ruby syntax but with a #, so that #{} is the empty map.
* M[K] looks up key K in map M, returns the corresponding value.
* Each binding in a map pattern is `expr => pattern`. All of the keys
  in the map value being matched must match one of the keys in the bindings.

Records.
--------
Internally, field names are represented by Symbols.
* `fields aRecord` returns a Set of Symbols.

Right now, we overload the `:` statement as <string>:<expr> and <ident>:<expr>.
New design, we use
    <symbol-expr> => <expr>
to generate a field from a symbol expression, and use
    <pattern> : <expr>
to generate fields by binding against a pattern.
This frees up string literals to be used as literal patterns.
* We also need quoted identifiers, to satisfy the use case of non-standard
  identifiers as field names.
* The need for => to construct fields with computed names is questionable,
  once maps are added.

Indexing Operations (See Optics)
--------------------------------
S!I
* list!integer | list![ix0,ix1,...]
* string!integer | string![ix0,ix1,...]
* record!symbol
  * vectorized indexing for records? Wait for more requirements.
    Maybe: record!set_of_symbols -> record
    Maybe: record!list_of_symbols -> list_of_values
* map!key
  * vectorized indexing is not possible: key can be a list or set.
* Motivations for this unusual syntax:
  * It's consistent across list, string, record and map.
    (Record doesn't support R[i] for indexing.)
    Might be useful for polymorphic operations like `fetch` or lenses?
  * Also, the `S!` syntax is potentially useful.
  * Unlike `fetch[i]S`, we can use `S!i` as a locative: `S!i := val`.

S!
Here ! is a postfix operator converting S to a function of an index value.

S[i,j,k,...]
* list[i,j,k] indexes a multidimensional array, can extract n-d slices.
* string[i] is same as string!i, map[k] is same as map!k.
  The reason for this redundancy is that this is the conventional syntax.
