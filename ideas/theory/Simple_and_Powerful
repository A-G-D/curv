Simple and Powerful
===================
The primary design goals for the Curv programming language are that it should
be simple, easy to use, and powerful. How do we make these goals precise,
and how do we achieve them? The issues are subtle and require deep analysis.

Simple is not the same as easy.
    See: "Simple Made Easy" -- Rich Hickey
    https://www.youtube.com/watch?v=oytL881p-nQ

Ease of use is discussed in another essay.
Here, we discuss simplicity and expressive power.
One way to achieve ease-of-use is to design a language that is already
familiar to users.

Simplicity, by Rob Pike
* Public Static Void <https://www.youtube.com/watch?v=5kj5ApnhPAE> 2010
    I'm always delighted by the light touch and stillness of early
    programming languages. Not much text; a lot gets done. Old programs
    read like quiet conversations between a well-spoken research worker
    and a well-studied mechanical collegue, not as a debate with a compiler.
    Who'd have guessed sophistication brought such noise? -- Richard Gabriel
  Modern languages have too much verbosity, too much bureaucracy, too many
  complex choices to make between overly-similar options.
* Simplicity is Complicated <https://www.youtube.com/watch?v=rFejpH_tAHM> 2015
  Sapir-Whorf hypothesis, language influences thought. This seems especially
  true for programming languages, which are far more diverse than natlangs.
  Mainstream languages are evolving by adding features from other languages.
  Complexity is growing while they become more similar to one another.
  The Go design process was different:
  * If a language has too many features, you waste time choosing which ones
    to use. Later, "why does the code work this way?". The code is harder to
    understand because it is written in a more complex language.
  * Preferable to have just one way, or fewer simpler ways to do things.
  * Features add complexity. We want simplicity.
  * Features hurt readability. Readability is paramount.
  * Look for a minimal, orthogonal, composable feature set that can express
    everything you need (as mentioned elsewhere here).
Curv, being a hybrid array/functional/imperative language, is not minimal.

A powerful language will surface the deepest principles underlying a subject,
revealing a new world to the user. When you learn such a language, you
internalize those principles, giving you more powerful ways of reasoning about
that world. 

Alan Kay, 2015: Power of Simplicity
https://www.youtube.com/watch?v=NdSD07U5uBs 10:00
[Epicycle story: the planets don't move in circles, but circles are simple,
so over thousands of years, astronomers built an increasingly complex theory
of planetary movement based on circles (epicycles). Ellipses are more complex
than circles, but they drastically simplify orbit theory.]
You get simplicity by finding a slightly more sophisticated building block.
It's when you go for a simple building block that anyone can understand
through common sense, that is when you start screwing yourself right and left,
because it might not be able to ramify through the degrees of freedom and
scaling that you need. It's this inability to fix the building blocks that is
one of the biggest problems computing has today.
[Doghouse story: anybody can build a doghouse, out of virtually any material,
(cardboard?), but those construction techniques do not scale.]
Incremental problem solving: start with a simple solution to a small problem,
then keep adding patches until you have an unmaintainable mess.
* Don't solve a problem, solve a context. Grand canyon story: you are so deep
  in your rut, you can't see anything in the outside world that is outside of
  your rut. It's adding a patch to your existing solution, while remaining
  inside your rut, vs getting out of the rut. Most problems are bogus, they
  come out of the current context, and you need to think your way out of the
  current context.
* The human body has 100 trillion cells (10 trillion are human). And it works.
  We can't build software with this many components, our methods don't scale.
  However, the internet scales. Decentralized/distributed control, loose
  coupling. Kay's biological metaphor for "cellular" software components.
  Functionality on the inside, services on the outside, a "cell wall"
  (encapsulation). More complicated than a data structure.
The internet has been scaled up by 10 orders of magnitude, has never been taken
down for maintenance.

Expressive power is tricky. In general, more power means more complexity.
    See: "On The Expressive Power of Programming Languages"
    https://www.youtube.com/watch?v=43XaZEn2aLc
The most powerful programming language is also the most complex: assembly
language.

Criteria for adding complexity, based on The Quality Without A Name.
* Usability - Will the feature make Curv more usable for novices?
  Is the feature something that developers will enjoy using?
  Would either group miss it if it was no longer available?
* Readability - Is the intent of the feature clear and well presented?
* Configurability - Can the user adapt the feature to his or her needs?
* Profoundness - Does the feature strike the user as special or unique,
  but at the same time, insightful and correct?

Considerations for adding complexity: Programmer Assistance.
The mechanisms for defining data types contribute maybe the bulk of the
complexity to modern programming languages. The design of Curv was to a large
extent motivated as a rebellion against the complexity of OOP and the Haskell
Type system. If we pare these features down to the bare minimum, we get
something like the K language, which has about 6 primitive types (numbers,
symbols, characters, lists, dictionaries & functions) and no mechanisms for
defining abstract data types. In such a language, the system doesn't know
anything about the application data types (they only exist in the programmer's
head, and in documentation). The structure added by modelling application data
types in the language provides benefits to the developer.

Curv should have a complexity budget. If a feature adds both power and
complexity to the language, then we need to find a balance between various
concerns:
 * The added complexity in learning the language initially.
 * The added complexity in using the language, in understanding the meaning
   of programs, given that this feature exists.
 * The amount of complexity that can be removed from programs by using the
   feature.

In general, there is a power vs complexity curve where greater power means
greater complexity (less simplicity).

  P |         *
  o |       *
  w |     *
  e |   *
  r | *
    +----------
     Complexity

This curve represents the sweet spots in language design space.
Most programming languages do not reside on this curve, but rather under it.
Most languages are weaker and more complicated than necessary.
 * The main culprit is this: If you are trying to fix a problem, it is easier
   to design a feature that just addresses the one specific use case you have
   in mind, without worrying about how it integrates with other features.
 * Popular languages become weaker and flabbier over time by the accretion of
   features while maintaining backward compatibility.
 * New languages are designed by copying popular, yet weak and flabby languages.
   This gives rise to a self-perpetuating cycle.

Fortunately, there are tricks for designing languages that are simple and
powerful.

Denotative semantics enables algebraic structure, which supports equational
reasoning and improves simplicity-power.
 * An algebra of programs: John Backus, "Can Programming be Liberated from
   the von Neumann Style?".
 * By designing an algebra of programs, the language designer is forced to
   make everything compose properly, and get all of the edge cases correct.
   This benefits developers even if they don't study the algebra
   or use it to reason formally.

Syntax is part of power.
 * “By relieving the brain of all unnecessary work, a good notation sets it
   free to concentrate on more advanced problems and in effect increases
   our mental power.” A. N. Whitehead
 * Kenneth E. Iverson, "Notation as a Tool of Thought".
 * Katherine Ye, "Notation and Thought"
   https://github.com/hypotext/notation

A well designed notation makes certain relationships more apparent: it makes
it easier to reason about the meaning of an expression, and it enables you
to perform algebraic symbol manipulation. Good syntax needs to be backed up
with algebraic structure at the semantic level. Enables algebraic reasoning
(also called equational reasoning). Some syntax examples:
 * infix notation aids in algebraic reasoning, especially the associative law.
 * De Bruijn notation enables reasoning and proofs about lambda expressions
   that are difficult to carry out using lambda notation (but you don't want
   to use De Bruijn notation all the time).

Simple, Compositional Semantics
-------------------------------
An important design goal for Curv is that it has simple, compositional
semantics. The meaning of a compound expression is determined by the meanings
of the subexpressions and by the rules used to combine them, in a simple way,
with a minimum of "spooky action at a distance".

That last bit means we want to minimize the ability of the context in which
an expression is written or executed to change its meaning in unexpected
and non-intuitive ways. This improves the effectiveness of local reasoning
about the meaning of a program.

The goal is to have semantics that are an order of magnitude simpler and
easier to understand than in conventional programming languages.

P. J. Landin coined the term "denotative semantics" to describe this goal.

    The commonplace expressions of arithmetic and algebra have a certain
    simplicity that most communications to computers lack. In particular,
    (a) each expression has a nesting subexpression structure, (b) each
    subexpression denotes something (usually a number, truth value or numerical
    function), (c) the thing an expression denotes, i.e., its "value", depends
    only on the values of its sub-expressions, not on other properties of them.

    An important distinction is the one between indicating what behavior,
    step-by-step, you want the machine to perform, and merely indicating
    what outcome you want. Put that way, the distinction will not stand up to
    close investigation. I suggest that the conditions (a-c) in Section 8 are
    a necessary part of "merely indicating what outcome you want." The word
    "denotative" seems more appropriate than nonprocedural, declarative or
    functional. The antithesis of denotative is "imperative." Effectively
    "denotative" means "can be mapped into ISWIM without using jumping or
    assignment," given appropriate primitives.

Here is Christopher Strachey, quoted in the same paper:

    The important characteristic of DLs is that it is possible to produce
    equivalence relations, particularly the rule for substitution which
    Peter Landin describes as β in his paper. That equivalance relation,
    which appears to be essential in almost every proof, does not hold if you
    allow assignment statements. The great advantage then of DLs is that they
    give you some hope of proving the equivalence of program transformations
    and to begin to have a calculus for combining and manipulating them.

Composability and Orthogonality
-------------------------------
A related idea is composability.

P.J. Landin made a persuasive case for composability
in his famous paper "The Next 700 Programming Languages", in 1964.
This paper also marked the beginning of the functional programming movement.

In Landin's analysis, a programming language consists of a basic set of
given things, plus a set of mechanisms for defining new things in terms of
existing things. These "mechanisms" are abstraction mechanisms, and they
include:
 * The ability to give a name to a thing.
 * The ability to define a functional abstraction.

Landin pointed out that typical programming languages tend to have many
different incompatible naming mechanisms,
and many different incompatible functional abstraction mechanisms,
each of which only applies to a narrow domain.
What was true in 1964 continues to be true today.

Programming languages could be much simpler and more powerful if there
was just a single naming mechanism, that could be used with any kind of thing,
and just a single functional abstraction mechanism, that could take any kind
of thing as an argument and return any kind of thing as a result!

The original designers of the Scheme programming language took up this
challenge. They wrote, in R3RS (1986):

    Programming languages should be designed not by piling feature on top
    of feature, but by removing the weaknesses and restrictions that make
    additional features appear necessary. Scheme demonstrates that a very
    small number of rules for forming expressions, with no restrictions
    on how they are composed, suffice to form a practical and efficient
    programming language that is flexible enough to support most of the
    major programming paradigms in use today.

This is also a primary design goal for Curv.

The R2RS Scheme report (1985) used more poetic language:

    Data and procedures and the values they amass,
    Higher-order functions to combine and mix and match,
    Objects with their local state, the messages they pass,
    A property, a package, the control point for a catch--
    In the Lambda Order they are all first class.
    One Thing to name them all, One Thing to define them,
    One Thing to place them in environments and bind them,
    In the Lambda Order they are all first-class.

Curv is a simple yet powerful language with a small number of orthogonal
building blocks that can be composed together in many ways.

Orthogonality means:
 * Unify features that are almost the same, but with unnecessary differences.
 * Split up features that do two two things at once into separate features
   that can be composed.

Composability means that every feature can be composed with every other
feature, with no restrictions other than logical or mathematical necessity.
