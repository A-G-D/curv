## TODO: Language Changes

* lib
* std
* a[i] := item, i is an integer
* remove a'b
* function/record equivalence
  * primitive functions have a 'call' field, support 'fields' and 'defined'
    and 'f.call' and 'include' and '...'.
  * 'is_record f' is true
  * 'is_fun r' is true if r is a record with a call field.
    is_fun x = defined(x.call);
* compose -- function composition. Short form is 'co'?
  x >> compose[f,g,h] means x>>f>>g>>h ?
  The function returned by compose reports a pattern match failure if any
  of the constituent functions reports a pattern match failure. This is
  important for `match` and *cast* patterns.
* any, all
* Record constructors may combine definitions and field generators.
  Shorter definitions for shape constructors with .mitred/.exact members.

## Ideas Under Development

### Patterns

New Pattern Types:
* '<pattern> if <condition>' -- conditional pattern. The pattern is matched,
  variables are bound, then the condition is evaluated over the bound variables,
  and the match fails if the condition is false.
  * The 'phrase abstraction' design pattern asks if the syntax for this should
    actually be 'if (condition) pattern'? But the C if syntax is
    counterintuitive because the pattern binds variables in the condition.
  * Also, should we have 'statement if condition'?
* 'pat1 && pat2' -- and pattern. The same value matches both patterns.
  * bound variables are disjoint between pat1 and pat2.
  * Idiom: 'id && pat' is like 'id@pat' in Haskell.
* 'pat1 || pat2' -- or pattern. The value matches either pat1 or pat2.
  * pat1 and pat2 define identical sets of bound variables.
  * Useful when simulating a switch statement (one case matching 2 values).
  * [x,y] || [x,y,_==0]
  * Rust, F# use 'pat1|pat2' -- looks nice in switch.
* 'cast >> pat' -- cast pattern.
  * If V is the value being matched, then we evaluate `cast V`: if cast's
    parameter pattern fails, the match fails. Otherwise, the result of `cast V`
    becomes the new value that is matched against `pat`.
  * A cast is a function that maps values onto a particular type, and is
    idempotent for members of that type. This feature implements implicit
    conversions to a type.
  * The parameter pattern match fails for values that are clearly not a member
    of the type. For values that are considered corrupted instances of the type,    the pattern match succeeds but an assertion fails.
  * S.P.Jones uses the term 'transformational pattern' (pat!fun), and shows
    that this is a way to reconcile pattern matching with data abstraction.
    The cast fun transforms an ADT to a pattern matchable value.
  * This wants to be a right-associative operator, but >> is left-associative.
  * Is 'cast1 >> (cast2 >> pat)' the same as 'compose[cast1,cast2] >> pat'?
    Only if the result of compose reports a pattern match failure if any
    of the constituent functions have a pattern match failure.
* Variations of predicate patterns:
  * 'pat == expr', 'pat != expr', 'pat < expr', ... -- relational op predicates
  * 'pat `elem` listexpr' -- range patterns, like 'x `elem` 1..10'.
  * Many languages have constant and range patterns, eg 42 and 1..10 are
    patterns, which works well in the 'switch statement' use case. But:
    * Conflicts with name abstraction, creates ambiguity.
      Is 'true' a constant pattern or an identifier pattern?
    * Conflicts with field_generator ::= "string":expr|pattern:expr

"Pattern Guards and Transformational Patterns", Simon Peyton Jones

If Pattern Matches Expression Then ...:
`if (pat <- expr) stmt1`
`if (pat <- expr) stmt1 else stmt2`
`<-` is pronounced "matches". `pat<-expr` is a pattern matching phrase,
which either succeeds (and binds some identifiers) or fails.
The bindings from pat are visible in stmt1.
There are compound PM phrases:
  PM ::= pat <- expr
  PM ::= PM1 && PM2 | PM1 && expr | expr && PM2
         // PM1 and PM2 must define disjoint bindings.
         // PM1 bindings are visible in PM2 and in expr.
         // 'expr' is equivalent to the PM '_ if expr <- null'.
  PM ::= PM1 || PM2
         // PM1 and PM2 must define coincident bindings.
         // PM || expr isn't legal because bindings from PM aren't
         // initialized if PM fails.

Guarded Expressions:
* Inspired by Haskell 2010 guarded equations, but more general.
* A 'guarded expression' is an expression that either yields a value or fails
  due to pattern match failure.
* If the body of a function is a guarded expression GE, then pattern
  match failure of the GE is pattern match failure of the function.
* `try function_call`
  `try function_call else expression`
  Eg, `try arg >> pat->result` is like: `if (pat <- arg) result`.
  `try` can be used to classify function arguments based on their parameter
  patterns.
* Syntax:
  GE ::= if (...) expr
       | if (...) GE
       | if (...) GE else GE
       | try expr
       | try expr else GE
       | expr
* Used with if(pat<-expr)...
* PM ::= pat <- GE
  PM fails if GE fails.
  Eg, if (r <- try f x) <do something with r if f x succeeds>

Filtered For Loop:
* Haskell: for (pattern in list) ..., list elements not matching pattern
  are skipped. Convenient in some cases, hides coding errors in others.
  Swift also has a syntax for this.
* In Curv, we can use
    for (x in list) if (pattern <- x) ...
  This is simple, and makes it obvious that filtering is happening.
* We could also incorporate the filtering into the for loop.
  I don't think it should happen by default, but maybe an alternate syntax:
    for (pattern <- list) ...
  Low priority, not clear we require such an abbreviation if the previous
  syntax is available.

I now have a general notion of a function call failing due to a domain
error, and the ability to direct evaluation based on this failure.
It reminds me of:
* "Goal directed evaluation" in Icon, with 'PM' expressions that
  behave like boolean conditions that can be combined with && and ||.
  Although that is supposed to also cover the generation of a stream of results,
  and backtracking on failure.
* Exception handling, although this particular variety is restricted so that
  it does not break referential transparency.
  
This is now very general. "pattern direc

### Unicode Strings
A string is a sequence of characters, but what is a character in Unicode?
 0. Legal code points.
    The NUL character is disallowed. Surrogate and noncharacter code points
    are disallowed.
 1. Character boundaries.
    Maybe a character is a code point. It's simple.
    In Swift, a character is an "extended grapheme cluster". Eg, an emoji
    flag character consists of two code points, but counts as one character.
    The 'string.count' method iterates over the string.
    The Unicode standard says that a "user perceived character"
    is most closely approximated by an "extended grapheme cluster".
    (Any closer approximation is locale-dependent.)
    http://unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries
    Regular expressions should ideally use EGCs as character boundaries.
    If I can find an open source library that implements these semantics,
    may I should use it.
     * https://github.com/michaelnmmeyer/grapheme
       It uses Ragel, which is similar to re2c.
       Ragel is in the Ubuntu Universe.
 2. When are two characters equal? If they have the same sequence of code
    points, or if they normalize to the same sequence? (How does Swift do it?)
    If two grapheme clusters are guaranteed (by the Unicode standard)
    to have the same printed representation, then they compare equal?
    * If two code point sequences are canonically equivalent, then they should
      test equal, independent of locale.
      https://en.wikipedia.org/wiki/Unicode_equivalence
    * Is there open source for comparing two unicode strings and determining
      canonical equivalence?
    * Is it easier or harder to pre-normalize all Curv strings and then
      test byte-level equality? If I support Unicode in symbols, then those
      should probably be normalized, for efficiency reasons.
 3. String Concatenation, Legal EGCs.
    Suppose S1 and S2 are strings.
    Is it guaranteed that count(strcat(S1,S2)) == count S1 + count S2?
    What if S1 is a latin base character, and S2 is a latin accent
    combining character? Should S2 even be treated as a legal character string?
    Suppose S1 and S2 are normalized. Is strcat(S1,S2) guaranteed to be
    normalized?

### Libraries.
* `lib` is a record for accessing library files in /usr/local/lib/curv.
  `lib.foo` loads /usr/local/lib/curv/foo.curv.
  * `include lib.foo`

### Type Classes:
We can define a 'type class' as a record containing a 'call' function
(a type predicate), plus constructors (eg 'make')
and operations on type instances. Eg,
    BBox = {
        call = ...;
        union = ...;
        intersection = ...;
    };
If multiple classes implement the same protocol, then we have the effect of
Haskell type classes. A class needs to be passed as an explicit argument.
    f T (T x) = ...

### Tensors
* rank t
* dimension t -- vector of dimensions of tensor t.

Tensor version of `map` or `for`, for tensorizing an operation.
Works on scalar (rank 0) operations, but also works on operations
with a well defined rank > 0. Eg, `phase` is a rank 1 operation.
* Something like:
    phase = tensor_map ((is_vec2 p)->atan2(p[Y],p[X]))
  to define a tensorized `phase` operation.
* Possible definition:
    tensor_map f =
      match[
        f,
        (is_list x)->[for (e in x) tensor_map f e],
      ];

Indexing:
* a[i] := item, i is an integer
* s.key := item
* remove a'b ?

List Slices:
`a[i..<count a]` is inconvenient, repeats `a`. Fix?
* Maybe a[...] is a special syntactic context, in which:
  * a[..j] and a[i..] are special abbreviations.
  * `end` is bound to index of final element, like MatLab.
    So a[i..end]
* Maybe `i..` is an infinite range value, can be used to slice a list.
  Short for `i..inf`. `count(i..)==inf`.
* `take n list` and `drop n list` for +ve and -ve n.
* Can't support Python negative indexes (-1 indexes the last element)
  because [0..count a - 1] is a list of the indexes of a, even if a is empty,
  meaning that [0..-1] denotes [].

Multidimensional array slicing a[i,j,k].
`a` is rectangular array of dimension `d`, or tree of max depth `d`.
Currently: Non-final indexes must be integers. Final index can be a tree.
Works on trees.

List Slice Assignment:
* `v[[X,Z]] := v2`, similar to GLSL vector swizzle assignment.
  Source and destination lists have the same count.
* a[i..j] := list. In a range slice assignment,
  source and destination lists can have different counts.
* Generalized `a[list1] := list2`. list1 and list2 can have different counts.
  If list1 is shorter, extra elements at end of list2 are appended to final
  element indexed by list1.
  If list1 is longer, extra elements indexed by list1 are deleted.

Multidimensional Array Slice Assignment `a[i,j,k] := x`.
`a` is rectangular array of dimension `d`, or tree of max depth `d`.
* All indices are integers. Easy, works for trees.
* `a` is an array, all indices are arrays. As in APL.
  What happens if the same element is indexed twice? What's the new value?

### Vectorized boolean operations:
* vectorize `bit`
* Vectorized relations are:
  * infix operators:
    * <' <=' >' >=' ==' !='
    * .< .<= .> .>= .== .!= (Julia)
  * or n-ary functions, infixed using backtick:
    * less, greater, equal, less_or_equal, greater_or_equal, not_equal (FLUENT)
    * lt, gt, eq, le, ge, ne (TERSE, Matlab)
    * less, greater, equal, not_less, not_greater, not_equal
    * If f'g is function composition, then we can write not'less, not'greater,
      not'equal? For binary, not ternary.
    * Hamcrest: greater_than, greater_than_or_equal_to,
                less_than, less_than_or_equal_to
                equal_to
    * Mathematica: equal, unequal, greater, greater_equal, less, less_equal
    * TensorFlow: equal, not_equal, greater, greater_equal, less, less_equal
    * eg, a `greater` b, a `gt` b, a `greater_than` b
          a `less_equal` b, a `less_or_equal` b, a `not_greater` b, a `le` b
          a `less_than_or_equal_to` b
    * unified table of alternative spellings:
      * eq, equal, equal_to
      * ne, not_equal, unequal, not_equal_to
      * lt, less, less_than
      * le, less_equal, less_or_equal, less_than_or_equal_to
* `not` is vectorized version of `!`.
* `all` and `any` are vectorized boolean monoids for && and ||.
  * Name is consistent with GLSL,R,Julia,Python,Matlab.
  * Use some(pred)list, every(pred)list, name consistent with Javascript.
  * Haskell uses "and". Eg, a `any` b  vs  a `and` b
* ifelse(cond,thenval,elseval)
* Rationale: vectorized boolean ops have different names than unvectorized ones.
  * x==y compares vectors and returns bool, unlike xs=='ys/xs `equal` ys.
  * a&&b short circuits if a==false, but all(a,b) must evaluate b if a==false
    to implement broadcasting.
  * ifelse(b,c,a) must evaluate all 3 arguments if b is an array.
  * The unvectorized relations are guaranteed to return Bool, or fail,
    which is appropriate for the most common use case, which is to construct
    an argument for if, &&, ||, which require a Bool argument.
* Many languages represent boolean values as the numbers 0 and 1.
  In an array language, an array of booleans represented as 0 and 1
  is a useful representation, you can do arithmetic on it.
  In Curv, booleans are not numbers, but you can convert between the two
  representations using vectorized operations. Eg,
  * enbool(i) = [false,true]'i;
  * debool(b) = ifelse(b, 1, 0);
  Or:
  * tobool(i), frombool(b)
  Or:
  * idiom: i != 0, or i !=' 0
  * bit(b) = ifelse(b, 1, 0);

### Debug actions:
* print_timing(string,expr), returns value of expr
* `debug identifier action` -- locally bind identifier in the scope of action
  to a debug object, used to query and modify the state of the debugger.
  This is a generalized debugger interface, which doesn't let debugger data
  or shared mutable state escape into the model. The debug object is a record,
  the individual named objects are perhaps top level bindings in the CLI when
  interacting with the debugger.
* `exec(expr)`: evaluate an expression for its side effects, discard results.
  `_=expr` could be an alternative (see _ pattern), but might be optimized away.
  * exec(file "unit_test.curv");
  * exec(print_timing("msg",expr));
* `enter_debugger`, aka `debug`
* assert_error(errorMessageString, expression)

### Generalized Definitions:
* `include {module}`
  Could be used to embed sequential bindings in a recursive scope, or v.v.?
  Could be useful in abstract evaluation where the {module} argument is
  unwrapped during abstract evaluation? Dunno.
  `include {module} where (localDefinitions)`? Information hiding, in lieu of
  private module members.
* `if (cond) (a=foo;b=bar) else (a=bar;b=foo)`
* `def where bindings`, let bindings in def`.
  Could be used to embed sequential bindings in a recursive scope, or v.v.
  Information hiding, in lieu of private module members.

### Lexical:
* unicode operators (needs re2c or Ragel)
    90° == 90*deg
    ≤ ≥ ≠
    ¬a == !a
    a·b == dot(a,b)
    a×b == a*b or maybe cross(a,b)
    a÷b
    √a
    a∧b == a&&b
    a∨b == a||b
    x→x+1   ==   x->x+1
    for (i ∈ 1..10)
    π == pi
    τ == tau
    ∞ == inf
    x↑y = x^y
    “foo” == "foo"  -- note, resistant to systems that autocorrect "" to “”
    g∘f == compose[g,f]
    1.0₁₀3 == 1.0e3 -- not seriously, though.
* Use _ as digit separator in numerals.
* Support ' (primes) in identifiers (like Haskell)? Nope, ' is needed elsewhere.
* Support `foo` quoted identifiers? Nope, ` already used for infix operators.
  Compatibility with GUI interfaces that display
  parameter names as GUI labels, JSON object keys.
* 'foo' quoted identifiers and x `f` y infix notation?

### OpenSCAD2 prototype oriented programming?
* Model parameters and shape are bundled into one value.
* Simple syntax for customizing a subset of model parameters, generating
  a new shape.
* The BOM feature: can extract model parameters from a model script,
  generate JSON output.
* Language support for using a GUI to tweak model parameters?
  `param bindings in shape`. But what does this construct? Are the parameters
  represented in the value that is constructed, or does the GUI interpret
  the syntax?
* CSG trees: output a CSG tree as JSON, read a CSG tree to reconstruct model.
  Shapes contain the name of their constructor, and their constructor
  arguments. (Implemented using a proposed 'constructor' feature.)

### Data Abstraction:
* Wm. Cook distinguishes two kinds of data abstraction: CLU style "ADT"s,
  and Smalltalk style "objects". The untyped lambda calculus is enough to
  do object oriented data abstraction. Record literals make it easier.
  http://www.cs.utexas.edu/~wcook/Drafts/2009/essay.pdf
* I don't need extra features to support CLU-style information hiding.
  It's not that important.
* I do need the ability to define shape "subclasses" that obey specialized
  protocols. Eg, symmetrical polyhedra that support the Conway operators.
  I can already do this: a shape protocol is defined informally as a set of
  shape fields and axioms. I don't need explicit classes or inheritance
  or other explicit language mechanisms.
* This approach uses "duck typing". A shape just has to obey the protocol,
  it doesn't have to explicitly declare that it obeys the protocol.
* Do I need a language mechanism that allows a shape or record to declare what
  protocols it supports, in support of a cheap protocol testing predicate?
  (Like `val isa protocol`.)
  * I like the idea. I want it for type declarations, Design By Contract, etc.
    And for overloading positional arguments based on type, rather than relying
    on parameter labels as in Smalltalk & Swift.
  * But it's complicated to design, implement, document.
  * Cook observes that Smalltalk programmers generally don't need or use such
    a feature. So it's likely I don't need this in Curv.
  * Cook observes a tradeoff between flexibility (duck typing) and type
    correctness (explicit typing). Academics prefer type correctness.
    Some dynamic language communities (eg Smalltalk) prefer flexibility.
* Thus: In 1.0, support labelled parameters, de-emphasize explicit typing
  and type predicates.
* By this logic, I can get rid of the built-in Shape type.
  There are now 7 types: the 6 JSON types, plus functions.
  A shape is a record that implements `is_2d`,`is_3d`,`dist`,`bbox`,`colour`.
  This is the "shape protocol". `make_shape` is implemented in Curv, defines
  missing fields using default values, aborts if a bad field is detected.
* Do I need "inheritance"? Ability to override data or function fields
  while maintaining self-reference in function fields. That's needed for OOP.
  Right now, I'll say no.

  Note if you build an object-like data abstraction using records, with
  fields representing instance variables, and fields representing methods
  that reference the instance variables (function closures that contain
  separate copies of said instance variables), then reassigning an instance
  variable will leave the methods out of sync with the instance variables.

  So use a programming idiom that avoids this. To update an instance variable,
  you actually transform a value into another related value, using a function
  that invokes the constructor with a transformed copy of the instance variables
  as arguments.

### Function/Record Equivalence.
* Using {call x = f x}, a record can behave like a function.
* For symmetry, a function behaves like a record with a single `call` field.
* What about `is_fun`?
  * Get rid of `is_fun`. `is_record` is true for primitive functions.
    Test for `call` field in a record to identify a function.
  * Or, `is_fun` is true for records that contain a `call` field,
    and `is_record` is true for primitive functions. So Fun is a subtype of
    Record.
    * Then, what about lists and strings, which also support function call
      syntax? Are List and String subtypes of Fun? If so, then `count` cannot
      be polymorphic across Record and List. We maybe need to use
      `count(fields record)` to count the fields in a record.

### Haskell algebraic type constructors
  data Maybe a = Nothing | Just a
Constructors for this type, in Curv:
  {Nothing:}     -- or `null`, to be idiomatic.
  {Just: 42}
New feature:
  The expression {foo:} is an abbreviation for {foo: true}.
  The pattern {foo:} matches the record {foo:true}, binds no parameters.
  Can't specify a default value for a field pattern like `foo:`.
Eg,
  align{x:{centre:}, z:{above:0}}
Eg,
  make_shape{is_3d:, ...}

### User defined infix operators.
Maybe `foo` also works as a low precedence prefix operator.
(`foo` b) a <==> (a `foo` b) <==> foo(a,b).
So you can also write:

gyroid
  >> shell .2
  >> lipshitz 2
  >> bend (tau*12)
  >> `smooth .1 .intersection` torus(tau*4, tau*2)
  >> colour (sRGB.HSV (1/3, 1, .5))

map (`sum` 1) list

### Indentation As Syntax
let:
  x=1
  y=2
in x+y

Also, `do:`, `where:`, `[:` and `{:`.

### Versioned External Packages
Like Rust Cargo. An external library has a GIT url and a version.
Eg, file{repo:"https://github.com/doug-moen/laser-curv.git", version:"1.0"}

Directory syntax for `file`.

### Action/Generator Abstraction
Support tail recursion as a form of iteration in list/record generators.
This could mean:
* New syntax: a recursive loop construct whose body can be an action or
  generator. Previously designed as `loop` (aka Scheme named let).
* Lambda abstraction for actions, list/record generators.

### Function Composition
Long-form function composition is `compose[f,g]`.
Maybe I want an abbreviated infix form?
    f'g
This looks nice if each function in the composition is an identifier.
The syntax makes sense if I create a library of standard function names
that are intended to be composed in this way.

Transformations can be composed.

Compose a colour map with an intensity field.
There are a bunch of standard named colour maps and intensity fields.
eg, rainbow'linear_gradient
    rainbow'radial_gradient
    greyscale'cosine_gradient

### Add shebang comments, `#!` to end of line
